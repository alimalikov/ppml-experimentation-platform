# filepath: c:\Users\alise\OneDrive\Desktop\Bachelor Thesis\ml_models\core\app.py
import streamlit as st
import pandas as pd
import os
import sys
import json
import io # Add this import
import importlib.util
import glob
import numpy as np
import traceback 
import ast # Ensure this is at the top of your app.py
import tempfile # Add this import for NamedTemporaryFile
from streamlit_ace import st_ace # Import for code editor component
import streamlit_ace
# Add these imports at the top of your file (after the existing imports)
from sklearn import datasets
from sklearn.datasets import make_classification, make_regression
from sklearn.datasets import fetch_openml # Ensure this is present
import warnings
from datetime import datetime  # Add this import for timestamps

# --- Example Plugin Snippets ---
EXAMPLE_PLUGIN_SNIPPETS = {
    "--- Select a Snippet ---": "", # Placeholder option, will be skipped by the viewer
    "Minimal Plugin Skeleton": """import streamlit as st
import pandas as pd
from typing import List, Dict, Any
import json
# For test environment, use ...base_anonymizer
# When saved, this will be converted to ..base_anonymizer
from ...base_anonymizer import Anonymizer

class MyNewPlugin(Anonymizer):
    def __init__(self):
        self._name = "My New Plugin" # TODO: Change Me
        self._description = "Description of what my new plugin does." # TODO: Change Me

    def get_name(self) -> str:
        return self._name

    def get_description(self) -> str:
        return self._description

    def get_sidebar_ui(self, all_cols: List[str], sa_col_to_pass: str | None, df_raw: pd.DataFrame | None, unique_key_prefix: str) -> Dict[str, Any]:
        st.sidebar.subheader(f"{self.get_name()} Configuration")
        # TODO: Add your UI elements here
        # Example: param = st.sidebar.text_input("My Parameter", key=f"{unique_key_prefix}_my_param")
        st.sidebar.info("Configure your plugin parameters here.")
        return {} # TODO: Return a dictionary of parameters

    def anonymize(self, df_input: pd.DataFrame, parameters: Dict[str, Any], sa_col: str | None) -> pd.DataFrame:
        df_anonymized = df_input.copy()
        st.info(f"'{self.get_name()}' anonymize method called. Implement your logic.")
        # TODO: Access parameters: my_param_value = parameters.get("my_param_key_from_get_sidebar_ui")
        # TODO: Modify df_anonymized here
        return df_anonymized

    def build_config_export(self, unique_key_prefix: str, sa_col: str | None) -> Dict[str, Any]:
        # TODO: Example: return {"my_param": st.session_state.get(f"{unique_key_prefix}_my_param")}
        return {}

    def apply_config_import(self, config_params: Dict[str, Any], all_cols: List[str], unique_key_prefix: str):
        # TODO: Example: st.session_state[f"{unique_key_prefix}_my_param"] = config_params.get("my_param")
        pass

    def get_export_button_ui(self, config_to_export: dict, unique_key_prefix: str):
        json_string = json.dumps(config_to_export, indent=4)
        st.sidebar.download_button(
            label=f"Export {self.get_name()} Config",
            data=json_string,
            file_name=f"{self.get_name().lower().replace(' ', '_')}_config.json",
            mime="application/json",
            key=f"{unique_key_prefix}_export_button"
        )

    def get_anonymize_button_ui(self, unique_key_prefix: str) -> bool:
        return st.button(f"Anonymize with {self.get_name()}", key=f"{unique_key_prefix}_anonymize_button")

def get_plugin():
    return MyNewPlugin() # TODO: Change Me
""",
    "Basic Redaction Plugin": """import streamlit as st
import pandas as pd
from typing import List, Dict, Any
import json
from ...base_anonymizer import Anonymizer # For test, use ...base_anonymizer

class BasicRedactionPlugin(Anonymizer):
    def __init__(self):
        self._name = "Basic Redactor"
        self._description = "Replaces values in a selected column with '[REDACTED]'."
    def get_name(self) -> str: return self._name
    def get_description(self) -> str: return self._description
    def get_sidebar_ui(self, all_cols: List[str], sa_col_to_pass: str | None, df_raw: pd.DataFrame | None, unique_key_prefix: str) -> Dict[str, Any]:
        st.sidebar.subheader(f"{self.get_name()} Config")
        col_to_redact = st.sidebar.selectbox("Column to Redact:", options=all_cols if all_cols else ["N/A"], key=f"{unique_key_prefix}_col_redact", help="Select the column whose values will be replaced.")
        return {"column_to_redact": col_to_redact}
    def anonymize(self, df_input: pd.DataFrame, parameters: Dict[str, Any], sa_col: str | None) -> pd.DataFrame:
        df_copy = df_input.copy()
        col = parameters.get("column_to_redact")
        if col and col != "N/A" and col in df_copy.columns:
            df_copy[col] = "[REDACTED]"
            st.info(f"Redacted column: {col}")
        elif col and col != "N/A":
            st.warning(f"Column '{col}' not found for redaction.")
        else:
            st.warning("No column selected for redaction or no columns available.")
        return df_copy
    def build_config_export(self, unique_key_prefix: str, sa_col: str | None) -> Dict[str, Any]:
        return {"column_to_redact": st.session_state.get(f"{unique_key_prefix}_col_redact")}
    def apply_config_import(self, config_params: Dict[str, Any], all_cols: List[str], unique_key_prefix: str):
        st.session_state[f"{unique_key_prefix}_col_redact"] = config_params.get("column_to_redact")
    def get_export_button_ui(self, config_to_export: dict, unique_key_prefix: str):
        st.sidebar.download_button(label=f"Export {self.get_name()} Config", data=json.dumps(config_to_export, indent=2), file_name=f"{self.get_name().lower().replace(' ', '_')}_config.json", mime="application/json", key=f"{unique_key_prefix}_export")
    def get_anonymize_button_ui(self, unique_key_prefix: str) -> bool:
        return st.button(f"Anonymize with {self.get_name()}", key=f"{unique_key_prefix}_anon_btn")

def get_plugin():
    return BasicRedactionPlugin()
""",
    "Numeric Scaler Plugin": """import streamlit as st
import pandas as pd
from typing import List, Dict, Any
import json
from ...base_anonymizer import Anonymizer # For test, use ...base_anonymizer

class BasicNumericScalerPlugin(Anonymizer):
    def __init__(self):
        self._name = "Numeric Scaler"
        self._description = "Scales a selected numeric column by a factor."
        self.default_factor = 1.0
    def get_name(self) -> str: return self._name
    def get_description(self) -> str: return self._description
    def get_sidebar_ui(self, all_cols: List[str], sa_col_to_pass: str | None, df_raw: pd.DataFrame | None, unique_key_prefix: str) -> Dict[str, Any]:
        st.sidebar.subheader(f"{self.get_name()} Config")
        numeric_cols = []
        if df_raw is not None:
            numeric_cols = df_raw.select_dtypes(include='number').columns.tolist()
        
        col_to_scale = st.sidebar.selectbox("Column to Scale:", options=numeric_cols if numeric_cols else ["N/A"], key=f"{unique_key_prefix}_col_scale", help="Select a numeric column to apply scaling.")
        scale_factor = st.sidebar.number_input("Scale Factor:", value=self.default_factor, step=0.1, key=f"{unique_key_prefix}_factor", help="Enter the factor to multiply the column values by.")
        return {"column_to_scale": col_to_scale, "scale_factor": scale_factor}
    def anonymize(self, df_input: pd.DataFrame, parameters: Dict[str, Any], sa_col: str | None) -> pd.DataFrame:
        df_copy = df_input.copy()
        col = parameters.get("column_to_scale")
        factor = parameters.get("scale_factor", self.default_factor)
        if col and col != "N/A" and col in df_copy.columns:
            try:
                df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce') * factor
                st.info(f"Scaled column '{col}' by factor {factor}.")
            except Exception as e:
                st.error(f"Error scaling column '{col}': {e}")
        elif col and col != "N/A":
            st.warning(f"Column '{col}' not found for scaling.")
        else:
            st.warning("No column selected for scaling or no numeric columns available.")
        return df_copy
    def build_config_export(self, unique_key_prefix: str, sa_col: str | None) -> Dict[str, Any]:
        return {"column_to_scale": st.session_state.get(f"{unique_key_prefix}_col_scale"), "scale_factor": st.session_state.get(f"{unique_key_prefix}_factor")}
    def apply_config_import(self, config_params: Dict[str, Any], all_cols: List[str], unique_key_prefix: str):
        st.session_state[f"{unique_key_prefix}_col_scale"] = config_params.get("column_to_scale")
        st.session_state[f"{unique_key_prefix}_factor"] = config_params.get("scale_factor", self.default_factor)
    def get_export_button_ui(self, config_to_export: dict, unique_key_prefix: str):
        st.sidebar.download_button(label=f"Export {self.get_name()} Config", data=json.dumps(config_to_export, indent=2), file_name=f"{self.get_name().lower().replace(' ', '_')}_config.json", mime="application/json", key=f"{unique_key_prefix}_export")
    def get_anonymize_button_ui(self, unique_key_prefix: str) -> bool:
        return st.button(f"Anonymize with {self.get_name()}", key=f"{unique_key_prefix}_anon_btn")

def get_plugin():
    return BasicNumericScalerPlugin()
"""
}

def validate_plugin_code(code_str: str, class_name_str: str) -> list:
    """
    Validates plugin code against common issues and requirements using AST.
    """
    results = []
    tree = None

    # 1. Check if code is empty
    if not code_str.strip():
        results.append({
            'status': 'âŒ Failed',
            'check': 'Code Presence',
            'message': 'No code provided. Plugin code cannot be empty.'
        })
        return results # Cannot proceed if code is empty

    # 2. Basic Python syntax check
    try:
        tree = ast.parse(code_str)
        results.append({
            'status': 'âœ… Passed',
            'check': 'Python Syntax',
            'message': 'Code is syntactically valid.'
        })
    except SyntaxError as e:
        results.append({
            'status': 'âŒ Failed',
            'check': 'Python Syntax',
            'message': f"Invalid syntax on line {e.lineno}: {e.msg}"
        })
        return results  # Stop validation if basic syntax is incorrect

    # Initialize flags for checks
    base_anonymizer_imported = False
    get_plugin_defined = False
    plugin_class_defined = False
    inherits_anonymizer = False
    defined_methods = set()

    # Expected elements
    # For permanent plugins, '..base_anonymizer' is standard.
    # For test environment, '...base_anonymizer' might be used due to temp_plugins depth.
    # We will check for either and guide if necessary.
    expected_import_module_level2 = "..base_anonymizer"
    expected_import_module_level3 = "...base_anonymizer"
    expected_import_name = "Anonymizer"
    
    expected_factory_function = "get_plugin"
    
    # These are methods that a typical plugin would override or implement from the Anonymizer interface
    # Add or remove methods based on your Anonymizer base class's abstract methods or expected interface
    expected_class_methods = {
        'get_name', 
        'get_description', 
        'get_sidebar_ui', 
        'anonymize',
        'build_config_export',
        'apply_config_import',
        'get_export_button_ui',
        'get_anonymize_button_ui'
        # __init__ is implicitly checked by class instantiation if get_plugin works
    }

    # 3. Traverse AST for detailed checks
    for node in ast.walk(tree):
        # Check for 'from ..base_anonymizer import Anonymizer' or 'from ...base_anonymizer import Anonymizer'
        if isinstance(node, ast.ImportFrom):
            is_level2_import = node.level == 2 and node.module == "base_anonymizer" # from ..base_anonymizer
            is_level3_import = node.level == 3 and node.module == "base_anonymizer" # from ...base_anonymizer
            
            if is_level2_import or is_level3_import:
                for alias in node.names:
                    if alias.name == expected_import_name:
                        base_anonymizer_imported = True
                        break
            if base_anonymizer_imported: # Found one, no need to check other ImportFrom nodes for this
                break 
                
    # Re-walk for other checks to ensure import is found first if it exists anywhere
    for node in ast.walk(tree):
        # Check for 'get_plugin' factory function
        if isinstance(node, ast.FunctionDef) and node.name == expected_factory_function:
            get_plugin_defined = True

        # Check for plugin class definition and its methods
        elif isinstance(node, ast.ClassDef) and node.name == class_name_str:
            plugin_class_defined = True
            # Check for Anonymizer inheritance
            for base in node.bases:
                if isinstance(base, ast.Name) and base.id == expected_import_name:
                    inherits_anonymizer = True
                    break
            # Collect defined methods within this class
            for sub_node in node.body:
                if isinstance(sub_node, ast.FunctionDef):
                    defined_methods.add(sub_node.name)

    # 4. Compile results based on findings

    # Base Anonymizer Import Check
    if base_anonymizer_imported:
        results.append({
            'status': 'âœ… Passed',
            'check': 'Base Anonymizer Import',
            'message': f"'{expected_import_name}' is imported from '{expected_import_module_level2}' or '{expected_import_module_level3}'."
        })
    else:
        results.append({
            'status': 'âŒ Failed',
            'check': 'Base Anonymizer Import',
            'message': f"Missing import: 'from ..base_anonymizer import {expected_import_name}' (recommended) or 'from ...base_anonymizer import {expected_import_name}'."
        })

    # get_plugin() Function Check
    if get_plugin_defined:
        results.append({
            'status': 'âœ… Passed',
            'check': f"'{expected_factory_function}()' Function",
            'message': 'Factory function is defined.'
        })
    else:
        results.append({
            'status': 'âŒ Failed',
            'check': f"'{expected_factory_function}()' Function",
            'message': f"Missing '{expected_factory_function}()' factory function."
        })

    # Plugin Class Definition Check
    if plugin_class_defined:
        results.append({
            'status': 'âœ… Passed',
            'check': f"Class '{class_name_str}' Definition",
            'message': f"Class '{class_name_str}' is defined."
        })

        # Anonymizer Inheritance Check (only if class is defined)
        if inherits_anonymizer:
            results.append({
                'status': 'âœ… Passed',
                'check': 'Anonymizer Inheritance',
                'message': f"Class '{class_name_str}' correctly inherits from '{expected_import_name}'."
            })
        else:
            results.append({
                'status': 'âŒ Failed', # This is crucial for functionality
                'check': 'Anonymizer Inheritance',
                'message': f"Class '{class_name_str}' does not appear to inherit from '{expected_import_name}'. Should be 'class {class_name_str}({expected_import_name}):'."
            })

        # Required Methods Check (only if class is defined)
        missing_methods = expected_class_methods - defined_methods
        if not missing_methods:
            results.append({
                'status': 'âœ… Passed',
                'check': 'Required Class Methods',
                'message': f"All expected methods ({', '.join(sorted(list(expected_class_methods)))}) are present in class '{class_name_str}'."
            })
        else:
            results.append({
                'status': 'âŒ Failed',
                'check': 'Required Class Methods',
                'message': f"Class '{class_name_str}' is missing methods: {', '.join(sorted(list(missing_methods)))}."
            })
            
    else: # Plugin class not defined
        results.append({
            'status': 'âŒ Failed',
            'check': f"Class '{class_name_str}' Definition",
            'message': f"Class '{class_name_str}' is not defined in the code."
        })
        # Cannot check inheritance or methods if class is not found
        results.append({
            'status': 'âŒ Failed',
            'check': 'Anonymizer Inheritance',
            'message': f"Cannot check inheritance; class '{class_name_str}' not found."
        })
        results.append({
            'status': 'âŒ Failed',
            'check': 'Required Class Methods',
            'message': f"Cannot check methods; class '{class_name_str}' not found."
        })

    # 5. Optional: Check for common anti-patterns or provide style warnings (e.g., use of print statements)
    # For example, a simple string check for 'print(' could be a warning
    if "print(" in code_str: # Basic check, could be refined with AST to avoid comments/strings
        results.append({
            'status': 'âš ï¸ Warning',
            'check': 'Use of print()',
            'message': "Found 'print()' statements. For user feedback in Streamlit, prefer 'st.write', 'st.info', 'st.warning', 'st.error', or logging."
        })
        
    return results


# --- THIS SHOULD BE THE FIRST STREAMLIT COMMAND ---
st.set_page_config(layout="wide")

# --- Add project root to sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# --- Import Base Anonymizer ---
try:
    from src.anonymizers.base_anonymizer import Anonymizer
except ImportError as e:
    st.error(f"CRITICAL ERROR: Could not import Anonymizer base class. Ensure 'src/anonymizers/base_anonymizer.py' exists. Error: {e}")
    st.stop()

# --- Anonymizer Plugin Loading & Session State for Test Plugin ---
ANONYMIZER_PLUGINS = {}
if "test_plugin_instance" not in st.session_state:
    st.session_state.test_plugin_instance = None
if "test_plugin_error" not in st.session_state:
    st.session_state.test_plugin_error = None
if "test_plugin_raw_code" not in st.session_state:
    st.session_state.test_plugin_raw_code = ""
if "test_plugin_class_name" not in st.session_state:
    st.session_state.test_plugin_class_name = ""
if "test_plugin_display_name" not in st.session_state:
    st.session_state.test_plugin_display_name = ""
if "plugin_editor_save_status" not in st.session_state:
    st.session_state.plugin_editor_save_status = ""
if "developer_mode_enabled" not in st.session_state:
    st.session_state.developer_mode_enabled = False
if "plugin_validation_results" not in st.session_state:
    st.session_state.plugin_validation_results = []
if "show_code_snippets_expander" not in st.session_state: # ADD THIS LINE
    st.session_state.show_code_snippets_expander = False     # ADD THIS LINE

# NEW: Session state for persisting uploaded file data
if "persisted_uploaded_file_bytes" not in st.session_state:
    st.session_state.persisted_uploaded_file_bytes = None
if "persisted_uploaded_file_name" not in st.session_state:
    st.session_state.persisted_uploaded_file_name = None
if "last_uploader_key_persisted" not in st.session_state: # To track which uploader's data is saved
    st.session_state.last_uploader_key_persisted = None

# NEW: Session state for persisted anonymized DataFrame
if 'df_anonymized_data' not in st.session_state:
    st.session_state.df_anonymized_data = None
if 'df_anonymized_source_technique' not in st.session_state: # To track which plugin generated it
    st.session_state.df_anonymized_source_technique = None

# Session State Bridge for ML App Integration
if 'ml_transfer_status' not in st.session_state:
    st.session_state.ml_transfer_status = None
if 'ml_transfer_notification' not in st.session_state:
    st.session_state.ml_transfer_notification = None
if 'datasets_collection' not in st.session_state:
    st.session_state.datasets_collection = {
        'original': None,
        'anonymized_datasets': {}  # {'technique_name': [dataset_entry1, dataset_entry2, ...]}
    }
if 'dataset_counters' not in st.session_state:
    st.session_state.dataset_counters = {}
if 'dataset_metadata' not in st.session_state:
    st.session_state.dataset_metadata = {}

# Add these to your session state initialization section

if "sample_dataset_info" not in st.session_state:
    st.session_state.sample_dataset_info = None
if "loaded_sample_dataset_name" not in st.session_state:
    st.session_state.loaded_sample_dataset_name = None


def validate_plugin_code(code_str, class_name_str):
    """Validate plugin code against common issues and requirements."""
    results = []
    
    # Check if code is empty
    if not code_str.strip():
        results.append({
            'status': 'âŒ Failed',
            'check': 'Code is not empty',
            'message': 'No code provided'
        })
        return results
    
    # Basic syntax check
    try:
        ast.parse(code_str)
        results.append({
            'status': 'âœ… Passed',
            'check': 'Python syntax',
            'message': ''
        })
    except SyntaxError as e:
        results.append({
            'status': 'âŒ Failed',
            'check': 'Python syntax',
            'message': f"Line {e.lineno}: {e.msg}"
        })
        return results  # Return early on syntax error
    
    # Check for required imports
    if "from ...base_anonymizer import Anonymizer" not in code_str:
        results.append({
            'status': 'âŒ Failed',
            'check': 'Base Anonymizer import',
            'message': "Missing 'from ...base_anonymizer import Anonymizer'"
        })
    else:
        results.append({
            'status': 'âœ… Passed',
            'check': 'Base Anonymizer import',
            'message': ''
        })
    
    # Check for get_plugin function
    if "def get_plugin" not in code_str:
        results.append({
            'status': 'âŒ Failed',
            'check': 'get_plugin() function',
            'message': "Missing 'def get_plugin()' factory function"
        })
    else:
        results.append({
            'status': 'âœ… Passed',
            'check': 'get_plugin() function',
            'message': ''
        })
    
    # Check if class exists in code
    if f"class {class_name_str}" not in code_str:
        results.append({
            'status': 'âŒ Failed',
            'check': f"Class '{class_name_str}' exists",
            'message': f"Class '{class_name_str}' not found in code"
        })
    else:
        results.append({
            'status': 'âœ… Passed',
            'check': f"Class '{class_name_str}' exists",
            'message': ''
        })
    
    # Check for required methods
    required_methods = ['anonymize', 'get_name', 'get_sidebar_ui']
    missing_methods = []
    
    for method in required_methods:
        if f"def {method}" not in code_str:
            missing_methods.append(method)
    
    if missing_methods:
        results.append({
            'status': 'âŒ Failed',
            'check': 'Required methods',
            'message': f"Missing: {', '.join(missing_methods)}"
        })
    else:
        results.append({
            'status': 'âœ… Passed',
            'check': 'Required methods',
            'message': ''
        })
    
    # Additional checks can be added here
    
    # Check for Anonymizer inheritance (warning only)
    if "class" in code_str and "(Anonymizer)" not in code_str:
        results.append({
            'status': 'âš ï¸ Warning',
            'check': 'Anonymizer inheritance',
            'message': "Class may not inherit from Anonymizer"
        })
    
    return results


def load_anonymizer_plugins(include_test_plugin=True):
    global ANONYMIZER_PLUGINS
    ANONYMIZER_PLUGINS = {} # Reset before loading
    plugin_dir_path = os.path.join(PROJECT_ROOT, "src", "anonymizers", "plugins")

    if not os.path.isdir(plugin_dir_path):
        st.warning(f"Plugin directory not found: {plugin_dir_path}. No file-based plugins will be loaded.")
    else:
        for filepath in glob.glob(os.path.join(plugin_dir_path, "*_plugin.py")):
            module_name = os.path.splitext(os.path.basename(filepath))[0]
            full_module_name = f"src.anonymizers.plugins.{module_name}"
            try:
                if full_module_name in sys.modules: # Ensure fresh import
                    del sys.modules[full_module_name]
                spec = importlib.util.spec_from_file_location(full_module_name, filepath)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    sys.modules[full_module_name] = module # Add to sys.modules for relative imports
                    spec.loader.exec_module(module)
                    if hasattr(module, "get_plugin") and callable(module.get_plugin):
                        plugin_instance = module.get_plugin()
                        if isinstance(plugin_instance, Anonymizer) and hasattr(plugin_instance, "get_name") and callable(plugin_instance.get_name):
                            plugin_name = plugin_instance.get_name()
                            if plugin_name in ANONYMIZER_PLUGINS:
                                st.warning(f"Duplicate plugin name '{plugin_name}' from {filepath}. Overwriting.")
                            ANONYMIZER_PLUGINS[plugin_name] = plugin_instance
                        else:
                            st.warning(f"Plugin from {filepath} invalid Anonymizer or missing get_name().")
                    else:
                        st.warning(f"Plugin file {filepath} missing 'get_plugin()' factory.")
                else:
                    st.warning(f"Could not create import spec for plugin {filepath}.")
            except Exception as e:
                st.error(f"Failed to load plugin from {filepath}: {e}")
                st.exception(e)

    if include_test_plugin and st.session_state.test_plugin_instance is not None and st.session_state.test_plugin_display_name:
        test_plugin_name_key = f"[TEST] {st.session_state.test_plugin_display_name}"
        if test_plugin_name_key in ANONYMIZER_PLUGINS:
            st.warning(f"Test plugin name '{test_plugin_name_key}' conflicts. May be overshadowed.")
        ANONYMIZER_PLUGINS[test_plugin_name_key] = st.session_state.test_plugin_instance

load_anonymizer_plugins()

# --- Helper function for XLSX download ---
@st.cache_data
def convert_df_to_xlsx(df):
    """Convert DataFrame to XLSX format for download"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Anonymized_Data')
    output.seek(0)
    return output.getvalue()

# --- Helper function for CSV download (keep existing one) ---
@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(index=False, sep=';').encode('utf-8')

# --- Session State Bridge Functions ---
def transfer_data_to_ml_app(original_df, anonymized_df, technique_name, sa_col):
    """
    Transfer original and anonymized data to ML app through session state bridge.
    This implements the Session State Bridge pattern for loose coupling between modules.
    """
    try:
        # Initialize the datasets_collection structure if needed
        if 'datasets_collection' not in st.session_state:
            st.session_state.datasets_collection = {
                'original': None,
                'anonymized_datasets': {}
            }
        
        # Set the original dataset
        st.session_state.datasets_collection['original'] = original_df.copy()
        st.session_state.df_uploaded = original_df.copy()  # ML app compatibility
        
        # Prepare anonymized dataset entry with metadata
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Initialize counter for this technique
        if technique_name not in st.session_state.dataset_counters:
            st.session_state.dataset_counters[technique_name] = 0
        
        # Increment counter
        st.session_state.dataset_counters[technique_name] += 1
        counter = st.session_state.dataset_counters[technique_name]
        
        # Create dataset entry with proper structure expected by ML app
        dataset_entry = {
            'dataframe': anonymized_df.copy(),
            'technique': technique_name,
            'sa_column': sa_col,
            'timestamp': timestamp,
            'dataset_id': f"{technique_name}_{counter}",
            'source': 'anonymization_app'
        }
        
        # Add to anonymized datasets collection
        if technique_name not in st.session_state.datasets_collection['anonymized_datasets']:
            st.session_state.datasets_collection['anonymized_datasets'][technique_name] = []
        
        st.session_state.datasets_collection['anonymized_datasets'][technique_name].append(dataset_entry)
        
        # Store metadata
        metadata_key = f"{technique_name}_{counter}"
        st.session_state.dataset_metadata[metadata_key] = {
            'technique': technique_name,
            'sa_column': sa_col,
            'original_shape': original_df.shape,
            'anonymized_shape': anonymized_df.shape,
            'timestamp': timestamp,
            'transfer_source': 'anonymization_app'
        }
        
        # Update transfer status
        st.session_state.ml_transfer_status = {
            'success': True,
            'timestamp': timestamp,
            'technique': technique_name,
            'original_rows': len(original_df),
            'anonymized_rows': len(anonymized_df),
            'dataset_id': dataset_entry['dataset_id']
        }
        
        return True, f"Successfully transferred {technique_name} dataset to ML app"
        
    except Exception as e:
        st.session_state.ml_transfer_status = {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        return False, f"Transfer failed: {str(e)}"

def get_transfer_status_display():
    """Get formatted transfer status for display"""
    if not st.session_state.ml_transfer_status:
        return None
    
    status = st.session_state.ml_transfer_status
    
    if status['success']:
        return {
            'type': 'success',
            'message': f"âœ… **Transfer Complete** - {status['technique']} dataset transferred to ML app",
            'details': f"ğŸ“Š Original: {status['original_rows']} rows â†’ Anonymized: {status['anonymized_rows']} rows | ğŸ• {status['timestamp']}"
        }
    else:
        return {
            'type': 'error',
            'message': f"âŒ **Transfer Failed** - {status.get('error', 'Unknown error')}",
            'details': f"ğŸ• {status['timestamp']}"
        }

# --- Config Import/Export Helper ---
def apply_imported_config_to_ui(cfg: dict, all_cols_from_df: list | None):
    if not isinstance(cfg, dict): raise ValueError("Config must be a dict.")
    if "technique" not in cfg: raise ValueError("Missing 'technique' in config.")

    technique_name = cfg["technique"]
    load_anonymizer_plugins() # Ensure plugins (esp. test plugin) are current
    plugin_to_apply = ANONYMIZER_PLUGINS.get(technique_name)

    if not plugin_to_apply:
        raise ValueError(f"Technique '{technique_name}' not available. Available: {list(ANONYMIZER_PLUGINS.keys())}")

    st.session_state["selected_technique_main"] = technique_name
    st.session_state["sa_col_selector_main"] = cfg.get("sa_col", "<None>") or "<None>"

    plugin_params = cfg.get("parameters", {})
    # Determine plugin_key_prefix for applying config
    plugin_key_prefix_base = plugin_to_apply.get_name().lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
    if technique_name.startswith("[TEST]") and st.session_state.test_plugin_display_name:
         # Use the display name for the test plugin's key prefix for consistency
        plugin_key_prefix = f"test_{st.session_state.test_plugin_display_name.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"
    else:
        plugin_key_prefix = plugin_key_prefix_base

    if all_cols_from_df is None:
         st.warning(f"Applying config for '{technique_name}' without loaded data. Column settings may not init correctly.")
    plugin_to_apply.apply_config_import(plugin_params, all_cols_from_df if all_cols_from_df else [], plugin_key_prefix)
    st.sidebar.success(f"Config for '{technique_name}' applied to UI.")


# --- Streamlit App UI ---
st.title("Modular Dataset Anonymization Tool")

# === SESSION STATE BRIDGE STATUS ===
# Show global transfer status at the top of the page
if st.session_state.ml_transfer_status:
    with st.container():
        transfer_status = get_transfer_status_display()
        if transfer_status:
            if transfer_status['type'] == 'success':
                st.success(transfer_status['message'])
                st.caption(transfer_status['details'])
            else:
                st.error(transfer_status['message'])
                st.caption(transfer_status['details'])
        
        # Quick action buttons
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            if st.button("ğŸ”„ Clear Status", key="clear_transfer_status"):
                st.session_state.ml_transfer_status = None
                st.rerun()
        with col2:
            if st.button("ğŸ“Š ML App", key="quick_ml_link", help="Quick reminder to access ML app"):
                st.info("ğŸ’¡ Run: `streamlit run ml_app.py` to access your transferred datasets")
        
        st.markdown("---")

# ========================= DEVELOPER MODE TOGGLE IN SIDEBAR ========================= #
st.sidebar.checkbox("Enable Plugin Developer Mode", key="developer_mode_enabled_checkbox",
                    on_change=lambda: setattr(st.session_state, 'developer_mode_enabled', st.session_state.developer_mode_enabled_checkbox))

# --- Button to show/hide code snippets ---
if st.sidebar.button("View Plugin Code Snippets", key="toggle_snippets_button"):
    st.session_state.show_code_snippets_expander = not st.session_state.show_code_snippets_expander

st.sidebar.markdown("---") # Separator

# ========================= CONFIG IMPORT / EXPORT UI (Still in Sidebar) ========================= #
st.sidebar.header("Configuration Import / Export")
if "pending_parsed_config" not in st.session_state: st.session_state.pending_parsed_config = None
if "pending_config_filename" not in st.session_state: st.session_state.pending_config_filename = None
if "last_uploaded_config_name_in_widget" not in st.session_state: st.session_state.last_uploaded_config_name_in_widget = None

imported_config_file = st.sidebar.file_uploader("Import configuration (JSON)", type=["json"], key="config_file_uploader_main")
if imported_config_file is not None:
    if imported_config_file.name != st.session_state.last_uploaded_config_name_in_widget:
        try:
            imported_config_file.seek(0)
            cfg_json_data = json.load(imported_config_file)
            st.session_state.pending_parsed_config = cfg_json_data
            st.session_state.pending_config_filename = imported_config_file.name
            st.session_state.last_uploaded_config_name_in_widget = imported_config_file.name
            st.sidebar.info(f"Config '{imported_config_file.name}' loaded. Click 'Apply'.")
        except Exception as e:
            st.sidebar.error(f"Failed to parse '{imported_config_file.name}': {e}")
            st.session_state.pending_parsed_config = None
elif imported_config_file is None and st.session_state.last_uploaded_config_name_in_widget is not None: # File removed
    st.session_state.pending_parsed_config = None
    st.session_state.pending_config_filename = None
    st.session_state.last_uploaded_config_name_in_widget = None

df_columns_for_config_apply = st.session_state.get('df_raw_columns_for_config_apply', None)
if st.session_state.pending_parsed_config and st.session_state.pending_config_filename:
    if st.sidebar.button(f"Apply Config: '{st.session_state.pending_config_filename}'", key="apply_imported_config_button"):
        try:
            apply_imported_config_to_ui(st.session_state.pending_parsed_config, df_columns_for_config_apply)
            st.session_state.pending_parsed_config = None
            st.session_state.pending_config_filename = None
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"Error applying config: {e}")
            st.exception(e)

# ========================= MAIN WIDGETS (Plugin-Driven - Selection in Sidebar) ========================= #
# ========================= CATEGORIZED PLUGIN SELECTION ========================= #

# Organize plugins by category
plugins_by_category = {}
for plugin_name, plugin_instance in ANONYMIZER_PLUGINS.items():
    try:
        category = plugin_instance.get_category() if hasattr(plugin_instance, 'get_category') else "Other"
    except Exception:
        category = "Other"
    
    if category not in plugins_by_category:
        plugins_by_category[category] = []
    plugins_by_category[category].append(plugin_name)

# Sort plugins within each category (Test plugins last within their category)
for category in plugins_by_category:
    plugins_by_category[category].sort(key=lambda x: (x.startswith("[TEST]"), x))

# Get all available categories
available_categories = list(plugins_by_category.keys())
available_categories.sort()

if not available_categories:
    st.error("No anonymization plugins loaded. Check 'src/anonymizers/plugins'.")
    st.stop()

# Initialize session state for category selection
if "selected_category" not in st.session_state:
    st.session_state.selected_category = available_categories[0]

st.sidebar.subheader("ğŸ·ï¸ Plugin Categories")
selected_category = st.sidebar.selectbox(
    "Select Category:",
    options=available_categories,
    key="plugin_category_selector",
    help="Choose a category to filter anonymization techniques"
)

# Get techniques for selected category
techniques_in_category = plugins_by_category.get(selected_category, [])

if not techniques_in_category:
    st.sidebar.error(f"No techniques available in category '{selected_category}'")
    st.stop()

# Show category description
category_descriptions = {
    "Privacy Models": "ğŸ”’ Classical privacy models like k-anonymity, l-diversity, t-closeness providing syntactic privacy guarantees",
    "Differential Privacy": "ğŸ“Š Formal privacy mechanisms providing mathematical privacy guarantees with calibrated noise",
    "Generative Models": "ğŸ¤– Machine learning models that generate synthetic data preserving statistical properties",
    "Perturbation Methods": "ğŸŒŠ Data modification techniques using noise addition and randomization for privacy",
    "Suppression & Generalization": "ğŸ“ Data hiding and generalization techniques for privacy protection",
    "Utility Preserving": "âš¡ Advanced techniques that maintain data utility while providing privacy",
    "Advanced Techniques": "ğŸ”¬ Cutting-edge privacy-preserving methods for specialized use cases",
    "Custom/Experimental": "ğŸ§ª User-defined or experimental anonymization methods",
    "Other": "ğŸ“¦ Miscellaneous anonymization techniques"
}

if selected_category in category_descriptions:
    st.sidebar.info(category_descriptions[selected_category])

# Show technique count in category
st.sidebar.caption(f"ğŸ“‹ {len(techniques_in_category)} technique(s) in this category")

# Technique selection within category
st.sidebar.subheader("ğŸ”§ Anonymization Techniques")

selected_technique_name = st.sidebar.selectbox(
    "Select Technique:",
    options=techniques_in_category,
    key="plugin_technique_selector",
    help="Choose a specific anonymization technique from the selected category"
)

current_plugin = ANONYMIZER_PLUGINS.get(selected_technique_name)

# Clear persisted anonymized data if the technique has changed from the one that generated it
if selected_technique_name != st.session_state.get('df_anonymized_source_technique'):
    st.session_state.df_anonymized_data = None
    st.session_state.df_anonymized_source_technique = None


if not current_plugin:
    st.error(f"Plugin '{selected_technique_name}' not loaded. This shouldn't happen.")
    st.stop()

st.header(f"{selected_technique_name} Technique Options") # Use selected_technique_name for header

# --- File Uploader ---
uploaded_file_key = "file_uploader_main_data"
uploaded_file = st.file_uploader("Upload your CSV or Excel file", type=["csv", "xlsx"], key=uploaded_file_key)
df_raw = None

# --- Sample Datasets Section ---
st.sidebar.markdown("---")
st.sidebar.header("ğŸ² Sample Datasets")
st.sidebar.markdown("*Try the platform with built-in datasets containing sensitive information*")

# === DATASET SIZE SELECTION ===
st.sidebar.markdown("**Choose Dataset Size:**")
dataset_size_options = {
    "ğŸ“Š Sample (Quick Test)": "sample",
    "ğŸ“ˆ Medium (Performance Test)": "medium", 
    "ğŸ“‰ Full (Real-world Scale)": "full"
}

selected_size = st.sidebar.selectbox(
    "Dataset size:",
    options=list(dataset_size_options.keys()),
    key="dataset_size_selector",
    help="Choose between sample (1x), medium (3x), or full (10x) dataset sizes"
)

dataset_size_key = dataset_size_options[selected_size]

# Show size info
size_descriptions = {
    "sample": "**Sample Size:** Quick testing with smaller datasets (1,000-2,000 rows)",
    "medium": "**Medium Size:** Performance testing with moderate datasets (3,000-6,000 rows)", 
    "full": "**Full Size:** Real-world scale testing with large datasets (10,000-20,000 rows)"
}

if dataset_size_key in size_descriptions:
    st.sidebar.info(size_descriptions[dataset_size_key])

# === CATEGORY FILTER (existing code) ===
st.sidebar.markdown("**Filter by Industry & Use Case:**")
dataset_categories = {
    "All Datasets": "all",
    "ğŸ”¬ Classic ML (Small & Real)": "classic_ml",
    "ğŸ¯ Binary Classification": "binary_classification", 
    "ğŸŒˆ Multi-class Classification": "multi_class",
    "ğŸ“ˆ Regression": "regression",
    "ğŸ­ Large Scale (10K+ samples)": "large_scale",
    "ğŸ¥ Healthcare & Medical": "healthcare",
    "ğŸ’° Financial & Banking": "financial",
    "ğŸ‘¥ Human Resources": "hr",
    "ğŸ“Š Research & Analytics": "research"
}

selected_category = st.sidebar.selectbox(
    "Choose dataset category:",
    options=list(dataset_categories.keys()),
    key="dataset_category_filter",
    help="Filter datasets by their industry domain and use case"
)

# Define comprehensive dataset categorization
if selected_category == "All Datasets":
    sample_dataset_options = {
        "None": None,
        # Classic ML
        "ğŸŒ¸ Iris Flowers": "iris",
        "ğŸ· Wine Classification": "wine", 
        "ğŸ—ï¸ Breast Cancer": "breast_cancer",
        "ğŸ  Boston Housing (Synthetic)": "boston_synthetic", # Renamed to clarify
        "ğŸ’‰ Diabetes Progression": "diabetes",
        "ğŸ”¢ Handwritten Digits": "digits",
        "ğŸ˜ï¸ California Housing": "california_housing",
        # Binary Classification
        "ğŸ’³ Credit Approval (Synthetic)": "credit_approval_synthetic",
        "ğŸš¢ Titanic Survival": "titanic",
        "â¤ï¸ Heart Disease (Cleveland)": "heart_disease_cleveland",
        "ğŸ“¢ Marketing Campaign (Synthetic)": "marketing_campaign_synthetic",
        # Multi-class
        "ğŸŒº Flower Species (Synthetic)": "flower_species_synthetic", # Differentiate from Iris
        "ğŸµ Music Genres (Synthetic)": "music_genre_synthetic",
        # Regression
        "ğŸš— Auto MPG": "auto_mpg",
        "ğŸ“ˆ Stock Prices (Synthetic)": "stock_prices_synthetic",
        # Large Scale
        "ğŸ’¼ Adult Income (Census)": "adult_income",
        # Healthcare
        "ğŸ¥ Medical Records": "medical_records", # Existing
        "ğŸ§ª Clinical Trials": "clinical_trials", # Existing
        # Financial
        "ğŸ’° Financial Transactions": "financial_transactions", # Existing
        # HR
        "ğŸ¢ Employee Records": "employee_records", # Existing
        # Research
        "ğŸ“Š Survey Responses": "survey_responses", # Existing
        "ğŸ‘¥ Customer Data": "customer_data" # Existing
    }
else:
    selected_cat_key = dataset_categories[selected_category]
    
    if selected_cat_key == "classic_ml":
        sample_dataset_options = {
            "None": None,
            "ğŸŒ¸ Iris Flowers": "iris",
            "ğŸ· Wine Classification": "wine", 
            "ğŸ—ï¸ Breast Cancer": "breast_cancer",
            "ğŸ  Boston Housing (Synthetic)": "boston_synthetic",
            "ğŸ’‰ Diabetes Progression": "diabetes",
            "ğŸ”¢ Handwritten Digits": "digits",
            "ğŸ˜ï¸ California Housing": "california_housing"
        }
    elif selected_cat_key == "binary_classification":
        sample_dataset_options = {
            "None": None,
            "ğŸ’³ Credit Approval (Synthetic)": "credit_approval_synthetic",
            "ğŸš¢ Titanic Survival": "titanic",
            "â¤ï¸ Heart Disease (Cleveland)": "heart_disease_cleveland",
            "ğŸ“¢ Marketing Campaign (Synthetic)": "marketing_campaign_synthetic"
        }
    elif selected_cat_key == "multi_class":
        sample_dataset_options = {
            "None": None,
            "ğŸŒ¸ Iris Flowers": "iris", # Iris is a classic multi-class
            "ğŸ”¢ Handwritten Digits": "digits", # Digits is also multi-class
            "ğŸŒº Flower Species (Synthetic)": "flower_species_synthetic",
            "ğŸµ Music Genres (Synthetic)": "music_genre_synthetic"
        }
    elif selected_cat_key == "regression":
        sample_dataset_options = {
            "None": None,
            "ğŸ  Boston Housing (Synthetic)": "boston_synthetic",
            "ğŸ’‰ Diabetes Progression": "diabetes",
            "ğŸ˜ï¸ California Housing": "california_housing",
            "ğŸš— Auto MPG": "auto_mpg",
            "ğŸ“ˆ Stock Prices (Synthetic)": "stock_prices_synthetic"
        }
    elif selected_cat_key == "large_scale":
        sample_dataset_options = {
            "None": None,
            "ğŸ’¼ Adult Income (Census)": "adult_income",
            "ğŸ˜ï¸ California Housing": "california_housing" # California housing is also fairly large
        }
    elif selected_cat_key == "healthcare":
        sample_dataset_options = {
            "None": None,
            "ğŸ—ï¸ Breast Cancer": "breast_cancer", # Medically relevant
            "ğŸ’‰ Diabetes Progression": "diabetes", # Medically relevant
            "â¤ï¸ Heart Disease (Cleveland)": "heart_disease_cleveland", # Medically relevant
            "ğŸ¥ Medical Records": "medical_records", # Existing
            "ğŸ§ª Clinical Trials": "clinical_trials"  # Existing
        }
    elif selected_cat_key == "financial":
        sample_dataset_options = {
            "None": None,
            "ğŸ’³ Credit Approval (Synthetic)": "credit_approval_synthetic",
            "ğŸ’° Financial Transactions": "financial_transactions" # Existing
        }
    elif selected_cat_key == "hr":
        sample_dataset_options = {
            "None": None,
            "ğŸ’¼ Adult Income (Census)": "adult_income", # Often used in HR contexts for salary prediction
            "ğŸ¢ Employee Records": "employee_records" # Existing
        }
    elif selected_cat_key == "research":
        sample_dataset_options = {
            "None": None,
            "ğŸ“Š Survey Responses": "survey_responses", # Existing
            "ğŸ‘¥ Customer Data": "customer_data"      # Existing
        }
    else:
        sample_dataset_options = {"None": None}

# Show comprehensive category info
if selected_category != "All Datasets":
    category_descriptions = {
        "ğŸ”¬ Classic ML (Small & Real)": "Well-known datasets from machine learning literature, mostly small and real-world.",
        "ğŸ¯ Binary Classification": "Datasets suitable for tasks predicting one of two outcomes (e.g., yes/no, true/false).",
        "ğŸŒˆ Multi-class Classification": "Datasets for tasks predicting one of several (more than two) categories.",
        "ğŸ“ˆ Regression": "Datasets for tasks predicting a continuous numerical value.",
        "ğŸ­ Large Scale (10K+ samples)": "Larger datasets to test performance and scalability, typically over 10,000 samples.",
        "ğŸ¥ Healthcare & Medical": "Medical records, clinical trials, pharmacy data with HIPAA-protected information or similar health data.",
        "ğŸ’° Financial & Banking": "Banking, loans, credit cards with PCI-DSS regulated financial data or similar financial info.",
        "ğŸ‘¥ Human Resources": "Employee records, payroll data with workplace privacy information or salary prediction tasks.",
        "ğŸ“Š Research & Analytics": "Survey responses, customer analytics with research participant data or general customer behavior."
    }
    
    if selected_category in category_descriptions:
        st.sidebar.info(f"**{selected_category}:** {category_descriptions[selected_category]}")

# Show filtered dataset count with industry context
total_datasets = len([k for k in sample_dataset_options.keys() if k != "None"])
if selected_category == "All Datasets":
    st.sidebar.caption(f"ğŸ“‹ Showing all {total_datasets} datasets across multiple industries")
else:
    st.sidebar.caption(f"ğŸ“‹ Showing {total_datasets} datasets in {selected_category}")

# --- Logic to handle file upload and persistence ---
# Try to restore df_raw if uploader is empty but we have persisted data for this uploader key
if uploaded_file is None and \
   st.session_state.persisted_uploaded_file_bytes is not None and \
   st.session_state.last_uploader_key_persisted == uploaded_file_key:
    
    file_bytes_io = io.BytesIO(st.session_state.persisted_uploaded_file_bytes)
    file_name_persisted = st.session_state.persisted_uploaded_file_name
    st.info(f"Using previously uploaded data: {file_name_persisted}") # Inform user
    try:
        if file_name_persisted.endswith('.csv'):
            df_raw = pd.read_csv(file_bytes_io, sep=None, engine='python')
        elif file_name_persisted.endswith('.xlsx'):
            df_raw = pd.read_excel(file_bytes_io, engine='openpyxl')
        # Note: uploaded_file object is still None. If downstream code strictly needs uploaded_file.name,
        # we'd use file_name_persisted.
    except Exception as e:
        st.error(f"Error re-processing persisted file '{file_name_persisted}': {e}")
        # Clear potentially corrupted persisted data
        st.session_state.persisted_uploaded_file_bytes = None
        st.session_state.persisted_uploaded_file_name = None
        st.session_state.last_uploader_key_persisted = None
        df_raw = None

elif uploaded_file is not None:
    # A new file is uploaded, or the file_uploader widget retained its state correctly.
    try:
        # Persist this newly uploaded file's content and name.
        uploaded_file.seek(0)
        st.session_state.persisted_uploaded_file_bytes = uploaded_file.getvalue()
        st.session_state.persisted_uploaded_file_name = uploaded_file.name
        st.session_state.last_uploader_key_persisted = uploaded_file_key
        
        # Clear any previous anonymized data as new raw data is being processed/re-processed
        st.session_state.df_anonymized_data = None
        st.session_state.df_anonymized_source_technique = None

        # Now, read df_raw from the current uploaded_file object for immediate use.
        uploaded_file.seek(0) # Reset pointer again for pandas to read
        if uploaded_file.name.endswith('.csv'):
            df_raw = pd.read_csv(uploaded_file, sep=None, engine='python')
        elif uploaded_file.name.endswith('.xlsx'):
            df_raw = pd.read_excel(uploaded_file, engine='openpyxl')
        
        # This success message was inside the 'if uploaded_file is not None:' block originally.
        # It should only show if df_raw was successfully loaded from the uploader.
        if df_raw is not None:
             st.success(f"Uploaded '{uploaded_file.name}' successfully!")

    except Exception as e:
        st.error(f"Error processing uploaded file: {e}")
        st.exception(e)
        df_raw = None
        # Clear persisted data if processing this new file failed
        st.session_state.persisted_uploaded_file_bytes = None
        st.session_state.persisted_uploaded_file_name = None
        st.session_state.last_uploader_key_persisted = None

# --- Main data processing block (if df_raw is available) ---
if df_raw is not None:
    sa_col_to_pass = None # Initialize sa_col_to_pass at the start of the block
    try:
        # This subheader and dataframe display should be here, active if df_raw exists
        st.subheader("Original Data Preview (First 5 rows)")
        st.dataframe(df_raw.head())
        all_cols = df_raw.columns.tolist()
        st.session_state['df_raw_columns_for_config_apply'] = all_cols

        # --- General Configuration (SA Column) ---
        st.sidebar.header("General Configuration")
        sa_col_options = ["<None>"] + all_cols
        current_sa_val = st.session_state.get("sa_col_selector_main", "<None>")
        sa_index = sa_col_options.index(current_sa_val) if current_sa_val in sa_col_options else 0
        
        sa_col_selected = st.sidebar.selectbox(
            "Select Sensitive Attribute (SA) column (optional):",
            options=sa_col_options, 
            index=sa_index, 
            key="sa_col_selector_main"
        )
        # This line re-assigns sa_col_to_pass based on selection, which is correct.
        sa_col_to_pass = None if sa_col_selected == "<None>" else sa_col_selected 

        # Initialize df_anonymized_current_run for this script run
        df_anonymized_current_run = None

        # --- Technique-Specific UI and Logic ---
        plugin_key_prefix_base = current_plugin.get_name().lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
        if selected_technique_name.startswith("[TEST]") and st.session_state.test_plugin_display_name:
            plugin_key_prefix_base = st.session_state.test_plugin_display_name.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
            plugin_key_prefix = f"test_{plugin_key_prefix_base}"
        else:
            plugin_key_prefix = plugin_key_prefix_base
        
        current_parameters = current_plugin.get_sidebar_ui(all_cols, sa_col_to_pass, df_raw, plugin_key_prefix)
        
        full_config_for_export = {
            "technique": selected_technique_name, 
            "sa_col": sa_col_to_pass,
            "parameters": current_plugin.build_config_export(plugin_key_prefix, sa_col_to_pass)
        }
        current_plugin.get_export_button_ui(full_config_for_export, plugin_key_prefix)

        if current_plugin.get_anonymize_button_ui(plugin_key_prefix): 
            if df_raw is not None and not df_raw.empty:
                with st.spinner(f"Applying {selected_technique_name}..."):
                    try:
                        # Calculate and immediately store in session state
                        anonymized_result = current_plugin.anonymize(df_raw.copy(), current_parameters, sa_col_to_pass)
                        st.session_state.df_anonymized_data = anonymized_result
                        st.session_state.df_anonymized_source_technique = selected_technique_name
                        df_anonymized_current_run = anonymized_result # Update current run variable
                    except Exception as e:
                        st.error(f"{selected_technique_name} Error: {e}")
                        st.exception(e)
                        st.session_state.df_anonymized_data = None # Clear on error
                        st.session_state.df_anonymized_source_technique = None
                        df_anonymized_current_run = None
            else:
                st.warning("Please upload a file before anonymizing.")
                # Clear stale anonymized data if raw data is gone or empty for this attempt
                st.session_state.df_anonymized_data = None
                st.session_state.df_anonymized_source_technique = None
                df_anonymized_current_run = None
        
        # Display logic uses df_anonymized_current_run
        if df_anonymized_current_run is not None and not df_anonymized_current_run.empty:
            st.success(f"{selected_technique_name} anonymization complete!")
            st.subheader(f"Anonymized Data Preview ({selected_technique_name})")
            st.dataframe(df_anonymized_current_run.head()) # Use df_anonymized_current_run
            
            # Change from CSV to XLSX download
            xlsx_anonymized = convert_df_to_xlsx(df_anonymized_current_run) # Use XLSX function
            
            current_file_name_for_download = st.session_state.persisted_uploaded_file_name
            if uploaded_file is not None: # Prefer current uploader name if available
                 current_file_name_for_download = uploaded_file.name
            elif not current_file_name_for_download: # Fallback if persisted is also somehow gone
                 current_file_name_for_download = "data"

            st.download_button(
                label=f"ğŸ“¥ Download Anonymized ({selected_technique_name}) Data as XLSX",  # Updated label
                data=xlsx_anonymized,  # Use XLSX data
                file_name=f"anonymized_{plugin_key_prefix_base}_{current_file_name_for_download.split('.')[0]}.xlsx",  # Changed extension
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",  # XLSX MIME type
                key=f"download_{plugin_key_prefix}",
            )
            
            # === SESSION STATE BRIDGE: Transfer to ML App ===
            st.markdown("---")
            st.subheader("ğŸ”— Transfer to ML Experimentation")
            st.markdown("*Send your original and anonymized datasets to the ML app for privacy-preserving machine learning experiments*")
            
            # Display transfer status if available
            transfer_status = get_transfer_status_display()
            if transfer_status:
                if transfer_status['type'] == 'success':
                    st.success(transfer_status['message'])
                    st.caption(transfer_status['details'])
                else:
                    st.error(transfer_status['message'])
                    st.caption(transfer_status['details'])
            
            # Transfer button with columns for layout
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button(
                    f"ğŸš€ Transfer to ML App", 
                    key=f"transfer_ml_{plugin_key_prefix}",
                    help="Transfer both original and anonymized datasets to the ML experimentation platform",
                    use_container_width=True
                ):
                    with st.spinner("Transferring datasets to ML app..."):
                        success, message = transfer_data_to_ml_app(
                            original_df=df_raw,
                            anonymized_df=df_anonymized_current_run,
                            technique_name=selected_technique_name,
                            sa_col=sa_col_to_pass
                        )
                        
                        if success:
                            st.success(message)
                            st.info("ğŸ’¡ **Next Steps:** Navigate to the ML app to start your privacy-preserving experiments!")
                            
                            # Show quick link to ML app if possible
                            st.markdown("""
                            **Quick Access:**
                            - Run the ML app: `streamlit run ml_app.py`
                            - Your datasets are now available in the ML app's "ğŸ“‚ Anonymized Data" tab
                            """)
                        else:
                            st.error(message)
                            
                        # Auto-refresh to show updated status
                        st.rerun()
            
            # Show dataset summary
            with st.expander("ğŸ“Š Dataset Transfer Summary", expanded=False):
                st.write("**Original Dataset:**")
                st.write(f"- Shape: {df_raw.shape}")
                st.write(f"- Columns: {', '.join(df_raw.columns.tolist())}")
                
                st.write("**Anonymized Dataset:**")
                st.write(f"- Shape: {df_anonymized_current_run.shape}")
                st.write(f"- Technique: {selected_technique_name}")
                st.write(f"- Sensitive Attribute: {sa_col_to_pass or 'None'}")
                
                # Show current ML app dataset collection status
                if st.session_state.datasets_collection['original'] is not None:
                    st.write("**Current ML App Status:**")
                    st.write(f"- Original dataset: âœ… Loaded ({st.session_state.datasets_collection['original'].shape[0]} rows)")
                    
                    total_anonymized = sum(len(datasets) for datasets in st.session_state.datasets_collection['anonymized_datasets'].values())
                    st.write(f"- Anonymized datasets: {total_anonymized} loaded")
                    
                    if st.session_state.datasets_collection['anonymized_datasets']:
                        for technique, datasets in st.session_state.datasets_collection['anonymized_datasets'].items():
                            st.write(f"  - {technique}: {len(datasets)} dataset(s)")
                else:
                    st.write("**ML App Status:** No datasets loaded yet")
        elif df_anonymized_current_run is not None and df_anonymized_current_run.empty:
            anonymize_button_key = f"{plugin_key_prefix}_anonymize_button" 
            # Check if the button was actually clicked in this run or if we are just showing empty persisted data
            if st.session_state.get(anonymize_button_key, False) or \
               (st.session_state.get('df_anonymized_data') is not None and st.session_state.df_anonymized_data.empty and \
                st.session_state.get('df_anonymized_source_technique') == selected_technique_name) :
                st.warning(f"Anonymization with {selected_technique_name} resulted in an empty DataFrame.")
    except Exception as e: # This is the except for "if df_raw is not None:"
        st.error(f"Error during data display or plugin UI setup: {e}")
        st.exception(e)
else: # This 'else' corresponds to 'if df_raw is not None:'
    st.info("Awaiting file upload to configure and anonymize.")
    if 'df_raw_columns_for_config_apply' in st.session_state:
        del st.session_state['df_raw_columns_for_config_apply']
    
    # Clear persisted anonymized data if raw data is gone
    st.session_state.df_anonymized_data = None
    st.session_state.df_anonymized_source_technique = None

    if st.session_state.get(uploaded_file_key) is None:
        st.session_state.persisted_uploaded_file_bytes = None
        st.session_state.persisted_uploaded_file_name = None
        st.session_state.last_uploader_key_persisted = None

# ========================= DEVELOPER TOOLS IN MAIN AREA (if enabled) ========================= #
if st.session_state.developer_mode_enabled:
    with st.expander("Developer Zone: Create & Test Plugins", expanded=True):
        st.subheader("Plugin Code Editor")

        # --- Snippet Loader and Apply Button REMOVED ---
        # st.selectbox(...) 
        # if selected_snippet_to_apply ... st.button(...)

        # st.markdown("---") # This separator can also be removed if desired

        current_code_for_editor = st.session_state.get("test_plugin_raw_code", "")
        code_returned_by_editor = st_ace(
            value=current_code_for_editor,
            language="python",
            theme="tomorrow_night",
            keybinding="vscode",
            font_size=14,
            height=600,
            show_gutter=True,
            wrap=True,
            auto_update=False,
            readonly=False,
            placeholder="""# Paste or type your Python plugin code here...
# You can view example snippets via the sidebar button.
# Required:
# - from ...base_anonymizer import Anonymizer
# - class YourPluginName(Anonymizer): ...
# - def get_plugin(): return YourPluginName()
# Implement all methods from the Anonymizer base class.
# Use st.sidebar for UI elements within get_sidebar_ui.
""",
            key="plugin_editor_code_area_ace_main" 
        )
        if code_returned_by_editor != current_code_for_editor:
            st.session_state.test_plugin_raw_code = code_returned_by_editor
        
        st.caption("Write your plugin code above. View example snippets via the sidebar. Ensure your code defines a class inheriting from `Anonymizer` and includes a `get_plugin()` factory function.")

        col1_dev, col2_dev = st.columns(2)
        with col1_dev:
            st.session_state.test_plugin_class_name = st.text_input(
                "Plugin Class Name (e.g., MyNewPlugin):", 
                value=st.session_state.get("test_plugin_class_name", ""), 
                key="plugin_editor_class_name_input_main"
            )
        with col2_dev:
            st.session_state.test_plugin_display_name = st.text_input(
                "Plugin Display Name (for dropdown):", 
                value=st.session_state.get("test_plugin_display_name", ""), 
                key="plugin_editor_display_name_input_main"
            )

        # "Validate Code" button
        if st.button("Validate Plugin Code", key="plugin_editor_validate_button"):
            st.session_state.plugin_validation_results = validate_plugin_code(
                st.session_state.test_plugin_raw_code,
                st.session_state.test_plugin_class_name
            )
            st.session_state.test_plugin_error = None
            st.session_state.plugin_editor_save_status = ""

        # Display Validation Results
        if st.session_state.get("plugin_validation_results"): # Use .get for safety
            st.markdown("---")
            st.subheader("Plugin Code Validation Checklist:")
            all_passed_or_warning = True # Track if all are either passed or warning
            has_failures = False

            for res in st.session_state.plugin_validation_results:
                icon = res['status'].split(" ")[0] # Get the icon part
                message = f" {res['check']}"
                if res['message']:
                    message += f" ({res['message']})"
                
                if "âŒ Failed" in res['status']:
                    st.error(icon + message)
                    all_passed_or_warning = False
                    has_failures = True
                elif "âš ï¸ Warning" in res['status']:
                    st.warning(icon + message)
                    # Warnings don't prevent the "all passed" for the toast, but they do prevent balloons if we kept them
                else: # Passed
                    st.success(icon + message)
            
            if not has_failures and any(r['status'] != 'âŒ Failed' for r in st.session_state.plugin_validation_results): # If no failures and at least one check ran
                 # st.balloons() # Old animation
                 st.toast("Validation successful! All checks passed.", icon="âœ…") # New professional toast

        if st.button("Test This Plugin In Session", key="plugin_editor_test_button"):
            st.session_state.plugin_validation_results = []
            st.session_state.test_plugin_instance = None
            st.session_state.test_plugin_error = None
            st.session_state.plugin_editor_save_status = ""
            
            code_str = st.session_state.test_plugin_raw_code
            class_name_str = st.session_state.test_plugin_class_name
            display_name_str = st.session_state.test_plugin_display_name

            if not all([code_str, class_name_str, display_name_str]):
                st.session_state.test_plugin_error = "Plugin Code, Class Name, and Display Name are required."
            else:
                # Attempt to load the test plugin using a temporary file to better handle imports
                temp_module_name = f"temp_test_plugin_{os.urandom(4).hex()}" # Unique temp module name
                temp_file_path = ""
                try:
                    # Create a temporary directory if it doesn't exist
                    temp_dir = os.path.join(PROJECT_ROOT, "src", "anonymizers", "plugins", "temp_plugins")
                    os.makedirs(temp_dir, exist_ok=True)

                    with tempfile.NamedTemporaryFile(
                        mode="w", 
                        suffix="_plugin.py", # Ensure it ends with _plugin.py if any loading logic depends on it
                        delete=False,        # We'll delete it manually after loading
                        dir=temp_dir,        # Place it within a known package structure
                        encoding="utf-8"
                    ) as tmp_file:
                        tmp_file.write(code_str)
                        temp_file_path = tmp_file.name
                    
                    # Construct a module name that importlib can understand relative to 'src'
                    # e.g., src.anonymizers.plugins.temp_plugins.temp_filename_without_py
                    relative_path_from_src = os.path.relpath(temp_file_path, PROJECT_ROOT)
                    module_spec_name_parts = os.path.splitext(relative_path_from_src)[0].split(os.sep)
                    # Ensure it's a valid Python module path (dots, no leading slashes)
                    module_spec_name = ".".join(part for part in module_spec_name_parts if part)


                    if module_spec_name in sys.modules: # Clean up if somehow loaded before
                        del sys.modules[module_spec_name]

                    spec = importlib.util.spec_from_file_location(module_spec_name, temp_file_path)
                    if not spec or not spec.loader:
                        raise ImportError(f"Could not create module spec for temporary plugin at {temp_file_path}")
                    
                    module = importlib.util.module_from_spec(spec)
                    sys.modules[module_spec_name] = module # Crucial for relative imports
                    spec.loader.exec_module(module)

                    if not hasattr(module, "get_plugin") or not callable(module.get_plugin):
                        raise AttributeError("Test plugin code is missing the 'get_plugin()' factory function.")
                    
                    temp_instance = module.get_plugin()

                    # Validate the instance (optional, but good practice)
                    if not isinstance(temp_instance, Anonymizer):
                        raise TypeError(f"The 'get_plugin()' function in the test code did not return an Anonymizer instance. Got: {type(temp_instance)}")
                    
                    # Check if the returned instance's class name matches the expected one (optional)
                    # This assumes get_plugin() returns an instance of the class_name_str
                    # This check might be too strict if get_plugin() instantiates a different class defined in the string
                    # if temp_instance.__class__.__name__ != class_name_str:
                    #    st.warning(f"Plugin's get_plugin() returned an instance of '{temp_instance.__class__.__name__}' instead of expected '{class_name_str}'.")


                    st.session_state.test_plugin_instance = temp_instance
                    st.success(
                        f"Plugin '{display_name_str}' (from code editor) loaded for testing! "
                        "It's now in the technique dropdown. "
                        "If your data disappeared, it should reload. Then, select the [TEST] plugin to use it."
                    )
                    load_anonymizer_plugins() 
                    st.rerun()

                except Exception as e:
                    st.session_state.test_plugin_error = f"Error loading test plugin: {e}\n{traceback.format_exc()}"
                finally:
                    if temp_file_path and os.path.exists(temp_file_path):
                        try:
                            os.remove(temp_file_path) # Delete the .py file
                        except Exception as e_del:
                            st.warning(f"Could not delete temporary test plugin file (.py) {temp_file_path}: {e_del}")
                    
                    # Attempt to delete the corresponding .pyc file if it exists alongside
                    # temp_file_path would be something like ".../temp_plugins/tmpXXXX_plugin.py"
                    if temp_file_path:
                        pyc_file_path_alongside = temp_file_path.replace(".py", f".cpython-{sys.version_info.major}{sys.version_info.minor}.pyc")
                        if os.path.exists(pyc_file_path_alongside):
                            try:
                                os.remove(pyc_file_path_alongside)
                            except Exception as e_del_pyc:
                                st.warning(f"Could not delete temporary .pyc file {pyc_file_path_alongside}: {e_del_pyc}")

                        # Attempt to delete from __pycache__ if it exists
                        # e.g. .../temp_plugins/__pycache__/tmpXXXX_plugin.cpython-XY.pyc
                        temp_dir_of_file = os.path.dirname(temp_file_path)
                        temp_file_basename = os.path.basename(temp_file_path)
                        pyc_filename_in_cache = temp_file_basename.replace(".py", f".cpython-{sys.version_info.major}{sys.version_info.minor}.pyc")
                        pyc_file_path_in_cache = os.path.join(temp_dir_of_file, "__pycache__", pyc_filename_in_cache)

                        if os.path.exists(pyc_file_path_in_cache):
                            try:
                                os.remove(pyc_file_path_in_cache)
                            except Exception as e_del_pyc_cache:
                                st.warning(f"Could not delete temporary .pyc file from __pycache__ {pyc_file_path_in_cache}: {e_del_pyc_cache}")
                        
                        # Optional: Clean up the __pycache__ directory itself if it's empty and belongs to temp_plugins
                        pycache_dir = os.path.join(temp_dir_of_file, "__pycache__")
                        if os.path.exists(pycache_dir) and not os.listdir(pycache_dir) and "temp_plugins" in pycache_dir:
                            try:
                                os.rmdir(pycache_dir)
                            except Exception as e_del_pycache_dir:
                                st.warning(f"Could not delete empty __pycache__ directory {pycache_dir}: {e_del_pycache_dir}")
                    
                    # The temp_dir (e.g. .../temp_plugins) itself is not deleted here,
                    # as it's a general workspace. Only the files related to the specific test run.
        
        if st.session_state.test_plugin_error: st.error(st.session_state.test_plugin_error)

        st.markdown("---")
        st.subheader("Save Tested Plugin")
        plugin_filename = st.text_input(
            "Filename (e.g., my_plugin.py):", 
            value=f"{st.session_state.test_plugin_display_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}_plugin.py" if st.session_state.test_plugin_display_name else "new_plugin.py",
            key="plugin_editor_filename_input"
        )
        if st.button("Save Plugin to File System", key="plugin_editor_save_button"):
            st.session_state.plugin_validation_results = [] # Clear validation on save attempt
            st.session_state.plugin_editor_save_status = ""
            code_to_save_original, fname = st.session_state.test_plugin_raw_code, plugin_filename # Get original code
            
            if not code_to_save_original: st.session_state.plugin_editor_save_status = "Error: No code to save."
            elif not fname: st.session_state.plugin_editor_save_status = "Error: Filename required."
            elif not fname.endswith("_plugin.py"): st.session_state.plugin_editor_save_status = "Error: Filename must end with '_plugin.py'."
            elif ".." in fname or "/" in fname or "\\" in fname: st.session_state.plugin_editor_save_status = "Error: Invalid filename characters."
            else:
                try:
                    plugin_dir = os.path.join(PROJECT_ROOT, "src", "anonymizers", "plugins")
                    if not os.path.exists(plugin_dir): os.makedirs(plugin_dir)
                    file_path = os.path.join(plugin_dir, fname)
                    
                    if os.path.exists(file_path):
                        st.session_state.plugin_editor_save_status = f"Warning: File '{fname}' exists. Not overwritten."
                    else:
                        # Modify the import path before saving
                        # This specifically targets the case where the test plugin might use three dots
                        # and the saved file (in src/anonymizers/plugins/) needs two dots.
                        code_to_save_modified = code_to_save_original.replace(
                            "from ...base_anonymizer import Anonymizer", 
                            "from ..base_anonymizer import Anonymizer"
                        )
                        # You might want to add a more specific check or a warning if other three-dot imports exist
                        # that are not related to base_anonymizer, but for now, this addresses the direct request.

                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(code_to_save_modified) # Save the potentially modified code
                        
                        st.session_state.plugin_editor_save_status = f"Success: Plugin saved as '{fname}'. Reloading..."
                        # Clear test plugin state after successful save
                        st.session_state.test_plugin_instance = None
                        st.session_state.test_plugin_display_name = ""
                        st.session_state.test_plugin_raw_code = "" # Clear editor
                        st.session_state.test_plugin_class_name = "" # Clear editor
                        load_anonymizer_plugins() # Reload all plugins
                        st.rerun()
                except Exception as e:
                    st.session_state.plugin_editor_save_status = f"Error saving plugin: {e}\n{traceback.format_exc()}"
        
        if st.session_state.plugin_editor_save_status:
            if "Success" in st.session_state.plugin_editor_save_status: st.success(st.session_state.plugin_editor_save_status)
            elif "Warning" in st.session_state.plugin_editor_save_status: st.warning(st.session_state.plugin_editor_save_status)
            else: st.error(st.session_state.plugin_editor_save_status)
else: # If developer mode is disabled
    st.session_state.plugin_validation_results = [] # Clear validation results
    if st.session_state.test_plugin_instance is not None: # Clear active test plugin
        st.session_state.test_plugin_instance = None
        st.session_state.test_plugin_display_name = ""
        load_anonymizer_plugins() # Reload to remove it from list
        st.rerun()

# === SESSION STATE BRIDGE ARCHITECTURE DOCUMENTATION ===
if st.session_state.developer_mode_enabled:
    with st.expander("ğŸ”— Session State Bridge Architecture", expanded=False):
        st.markdown("""
        ### ğŸ“‹ Session State Bridge Pattern
        
        This application implements a **Session State Bridge** pattern for loose coupling between the 
        anonymization and ML experimentation modules. This architectural choice provides:
        
        #### ğŸ¯ **Key Benefits:**
        - **Loose Coupling**: Modules operate independently without direct dependencies
        - **Flexible Communication**: Data flows through a centralized session state repository
        - **Extensibility**: Easy to add new modules or modify existing ones
        - **Robust Design**: Prevents tight coupling that could break when modules change
        
        #### ğŸ”§ **Implementation Details:**
        
        **Data Transfer Flow:**
        1. **Anonymization App** â†’ Session State Bridge â†’ **ML App**
        2. Original dataset stored in `st.session_state.datasets_collection['original']`
        3. Anonymized datasets stored in `st.session_state.datasets_collection['anonymized_datasets']`
        4. Metadata tracked in `st.session_state.dataset_metadata`
        
        **Session State Structure:**
        ```python
        st.session_state.datasets_collection = {
            'original': DataFrame,
            'anonymized_datasets': {
                'technique_name': [dataset_entry1, dataset_entry2, ...]
            }
        }
        ```
        
        **Dataset Entry Format:**
        ```python
        dataset_entry = {
            'dataframe': pd.DataFrame,
            'technique': str,
            'sa_column': str,
            'timestamp': str,
            'dataset_id': str,
            'source': 'anonymization_app'
        }
        ```
        
        #### ğŸš€ **Usage Pattern:**
        1. Upload and anonymize data in this app
        2. Click "Transfer to ML App" to bridge data
        3. Open ML app (`streamlit run ml_app.py`)
        4. Find transferred data in "ğŸ“‚ Anonymized Data" tab
        5. Run privacy-preserving ML experiments
        
        #### ğŸ” **Monitoring:**
        - Transfer status shown in sidebar and main area
        - Dataset counts and metadata tracked
        - Clear buttons for data management
        """)

# --- Ensure plugin directories exist on startup ---
plugin_base_dir = os.path.join(PROJECT_ROOT, "src", "anonymizers", "plugins")
os.makedirs(plugin_base_dir, exist_ok=True)

# Specific check for mondrian_plugin.py presence
mondrian_plugin_path = os.path.join(plugin_base_dir, "mondrian_plugin.py")
if not os.path.isfile(mondrian_plugin_path):
    st.warning(f"Expected plugin file not found: {mondrian_plugin_path}. Ensure the file exists.")

# Ensure temp_plugins directory exists (app should create this too)
temp_plugins_dir = os.path.join(plugin_base_dir, "temp_plugins")
os.makedirs(temp_plugins_dir, exist_ok=True)

# Inform about the temp_plugins directory
st.info(f"Temporary plugins directory: {temp_plugins_dir}. This is where test plugins are loaded from.")

# --- Example plugin snippets (read-only and copyable) ---
# This section is controlled by the show_code_snippets_expander state
if st.session_state.get("show_code_snippets_expander", False):
    with st.expander("Plugin Code Snippets (Read-Only & Copy)", expanded=True):
        st.markdown("Below are some example plugin snippets. You can manually select and copy the code to paste into the editor in the 'Developer Zone'.")
        
        # The EXAMPLE_PLUGIN_SNIPPETS dictionary should be defined earlier in your script
        # (e.g., near the top, after imports).
        if 'EXAMPLE_PLUGIN_SNIPPETS' in globals() and isinstance(EXAMPLE_PLUGIN_SNIPPETS, dict):
            for snippet_name, snippet_code in EXAMPLE_PLUGIN_SNIPPETS.items():
                if snippet_name == "--- Select a Snippet ---": # Skip placeholder
                    continue
                st.subheader(snippet_name)
                st.code(snippet_code, language="python")
                st.markdown("---") # Separator between snippets
        else:
            st.warning("EXAMPLE_PLUGIN_SNIPPETS dictionary not found or not a dictionary.")
        
        if st.button("Close Snippet Viewer", key="close_snippet_viewer_button"):
            st.session_state.show_code_snippets_expander = False
            st.rerun() # Rerun to immediately hide the expander

# Add these functions after your imports and before the main UI code

@st.cache_data
def load_sample_dataset(dataset_name, dataset_size="sample"):
    """Load sample datasets with size options"""
    import pandas as pd
    import numpy as np
    
    try:
        np.random.seed(42)  # For reproducible data
        
        # Define size multipliers
        if dataset_size == "sample":
            size_multiplier = 1
        elif dataset_size == "medium":
            size_multiplier = 3
        elif dataset_size == "full":
            size_multiplier = 10
        else:
            size_multiplier = 1
        
        df = None

        # === Scikit-learn Classic Datasets ===
        if dataset_name == "iris":
            data = datasets.load_iris(as_frame=True)
            df = data.frame
            if size_multiplier > 1 and len(df) < 500: # Replicate small datasets
                df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle
        elif dataset_name == "wine":
            data = datasets.load_wine(as_frame=True)
            df = data.frame
            if size_multiplier > 1 and len(df) < 600:
                df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        elif dataset_name == "breast_cancer":
            data = datasets.load_breast_cancer(as_frame=True)
            df = data.frame
            if size_multiplier > 1 and len(df) < 2000:
                 df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                 df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        elif dataset_name == "diabetes":
            data = datasets.load_diabetes(as_frame=True)
            df = data.frame
            if size_multiplier > 1 and len(df) < 1500:
                df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        elif dataset_name == "digits":
            data = datasets.load_digits(as_frame=True)
            df = data.frame
            if size_multiplier > 1 and len(df) < 6000: # Digits is a bit larger
                df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        elif dataset_name == "california_housing":
            data = datasets.fetch_california_housing(as_frame=True)
            df = data.frame # This is already large, multiplier won't replicate
            # For very large datasets, multiplier could mean taking a larger sample if supported,
            # but here we load the full set. Info will reflect actual size.

        # === Synthetic Datasets (based on ML archetypes) ===
        elif dataset_name == "boston_synthetic": # Synthetic version for Boston Housing
            n_samples = 506 * size_multiplier # Original Boston had 506 samples
            n_features = 13
            X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=10,
                                   noise=20.0, random_state=42)
            df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_features)])
            df['target_price'] = y
            # Add some plausible PII-like columns for anonymization demo
            df['owner_name_initials'] = [f"{chr(65+np.random.randint(0,26))}{chr(65+np.random.randint(0,26))}" for _ in range(n_samples)]
            df['street_number_hash'] = [hash(f"Street {i}") % 10000 for i in range(n_samples)]

        elif dataset_name == "credit_approval_synthetic":
            n_samples = 690 * size_multiplier # Similar to UCI credit approval
            X, y = make_classification(n_samples=n_samples, n_features=15, n_informative=8,
                                       n_redundant=2, n_clusters_per_class=2, random_state=42)
            df = pd.DataFrame(X, columns=[f'attr_{i+1}' for i in range(15)])
            df['approved'] = y
            df['applicant_id'] = [f"APP-{1000+i}" for i in range(n_samples)]
            df['zip_code_prefix'] = np.random.randint(100, 999, n_samples)


        elif dataset_name == "marketing_campaign_synthetic":
            n_samples = 2240 * size_multiplier # Inspired by some marketing datasets
            X, y = make_classification(n_samples=n_samples, n_features=10, n_informative=5, random_state=42)
            df = pd.DataFrame(X, columns=[f'factor_{i+1}' for i in range(10)])
            df['subscribed'] = y
            df['customer_email_domain'] = np.random.choice(['@gmail.com', '@yahoo.com', '@hotmail.com', '@company.com'], n_samples)
            df['last_contact_days_ago'] = np.random.randint(1, 365, n_samples)

        elif dataset_name == "flower_species_synthetic": # Generic multi-class
            n_samples = 300 * size_multiplier
            X, y = make_classification(n_samples=n_samples, n_features=4, n_informative=3, n_classes=3, 
                                       n_clusters_per_class=1, random_state=42)
            df = pd.DataFrame(X, columns=['sepal_length_syn', 'sepal_width_syn', 'petal_length_syn', 'petal_width_syn'])
            df['species_type'] = y
            df['observation_id'] = [f"OBS-{2000+i}" for i in range(n_samples)]

        elif dataset_name == "music_genre_synthetic":
            n_samples = 1000 * size_multiplier
            X, y = make_classification(n_samples=n_samples, n_features=20, n_informative=10, n_classes=5, random_state=42)
            df = pd.DataFrame(X, columns=[f'audio_feature_{i+1}' for i in range(20)])
            df['genre'] = y
            df['track_id_suffix'] = [f"-TRK{np.random.randint(1000,9999)}" for _ in range(n_samples)]

        elif dataset_name == "stock_prices_synthetic":
            n_samples = 500 * size_multiplier
            # Simulate some features that might influence stock price
            X_reg, y_reg = make_regression(n_samples=n_samples, n_features=5, n_informative=3, noise=10, random_state=42)
            df = pd.DataFrame(X_reg, columns=['market_sentiment', 'prev_volume', 'interest_rate', 'news_score', 'day_of_week'])
            df['price_change'] = y_reg
            df['company_ticker_generic'] = np.random.choice(['AAA', 'BBB', 'CCC', 'DDD'], n_samples)
            df['trade_date_offset'] = range(n_samples)


        # === Datasets from OpenML ===
        elif dataset_name == "titanic":
            try:
                data = fetch_openml(name='titanic', version=1, as_frame=True, parser='auto')
                df = data.frame
                # Titanic is relatively small, allow replication
                if size_multiplier > 1 and len(df) < 3000:
                     df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                     df = df.sample(frac=1, random_state=42).reset_index(drop=True)
            except Exception as e:
                st.error(f"Failed to fetch Titanic dataset from OpenML: {e}")
                return None
        elif dataset_name == "heart_disease_cleveland": # Using Cleveland Heart Disease from OpenML
            try: # data_id for Cleveland Heart Disease on OpenML is 40945, but sometimes IDs change.
                 # A common one is also 'heart-disease' name with specific data_id.
                 # Let's try by name first, then a known data_id as fallback.
                data = fetch_openml(name='heart-disease', version=1, as_frame=True, parser='auto') # UCI Heart Disease dataset
                df = data.frame
                if size_multiplier > 1 and len(df) < 1000:
                     df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                     df = df.sample(frac=1, random_state=42).reset_index(drop=True)
            except Exception as e_name:
                try: # Fallback to a known data_id if name fails
                    data = fetch_openml(data_id=53, as_frame=True, parser='auto') # statlog heart, another common one
                    df = data.frame
                    if size_multiplier > 1 and len(df) < 1000:
                        df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
                except Exception as e_id:
                    st.error(f"Failed to fetch Heart Disease dataset from OpenML by name ('{e_name}') or ID ('{e_id}').")
                    return None
        elif dataset_name == "auto_mpg":
            try:
                data = fetch_openml(name='autoMpg', version=1, as_frame=True, parser='auto')
                df = data.frame
                if size_multiplier > 1 and len(df) < 1200:
                     df = pd.concat([df.copy() for _ in range(size_multiplier)], ignore_index=True)
                     df = df.sample(frac=1, random_state=42).reset_index(drop=True)
            except Exception as e:
                st.error(f"Failed to fetch Auto MPG dataset from OpenML: {e}")
                return None
        elif dataset_name == "adult_income": # Census income dataset
            try:
                data = fetch_openml(name='adult', version=2, as_frame=True, parser='auto') # version 2 is common
                df = data.frame # This is large, multiplier won't replicate
            except Exception as e:
                st.error(f"Failed to fetch Adult Income dataset from OpenML: {e}")
                return None

        # === EXISTING DATASETS WITH SCALABLE SIZES (Copied from original, ensure they are still relevant) ===
        elif dataset_name == "customer_data":
            n_samples = 1000 * size_multiplier
            # ... (rest of customer_data generation code - keeping it concise here)
            age = np.random.normal(40, 15, n_samples).clip(18, 80)
            income = np.random.exponential(50000, n_samples) + age * 1000
            credit_score = np.random.normal(650, 100, n_samples).clip(300, 850)
            first_names = ['John', 'Jane', 'Michael', 'Sarah', 'David', 'Emily', 'Robert', 'Lisa'] * (size_multiplier * 125 // 8 +1)
            last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis'] * (size_multiplier * 125 // 8+1)
            df = pd.DataFrame({
                'customer_id': range(1, n_samples + 1),
                'first_name': np.random.choice(first_names, n_samples, replace=True),
                'last_name': np.random.choice(last_names, n_samples, replace=True),
                'age': age.astype(int),
                'annual_income': income.round(2),
                'credit_score': credit_score.astype(int),
                'phone_number': [f"555-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}" for _ in range(n_samples)],
                'email': [f"user{i}@example.com" for i in range(n_samples)], # Changed domain
                'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),
                'zip_code': np.random.randint(10000, 99999, n_samples)
            })
            
        elif dataset_name == "employee_records":
            n_samples = 800 * size_multiplier
            # ... (rest of employee_records generation code)
            departments = ['Engineering', 'Marketing', 'Sales', 'HR', 'Finance']
            positions = ['Manager', 'Senior', 'Junior', 'Lead', 'Analyst']
            df = pd.DataFrame({
                'employee_id': [f"EMP{str(i).zfill(6)}" for i in range(1, n_samples + 1)],
                'full_name': [f"Employee {i}" for i in range(n_samples)],
                'department': np.random.choice(departments, n_samples),
                'position': np.random.choice(positions, n_samples),
                'salary': np.random.normal(75000, 25000, n_samples).clip(30000, 200000).round(2),
                'years_experience': np.random.exponential(5, n_samples).clip(0, 40).astype(int),
                'performance_rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.15, 0.4, 0.3, 0.1]),
                'ssn': [f"{np.random.randint(100, 999)}-{np.random.randint(10, 99)}-{np.random.randint(1000, 9999)}" for _ in range(n_samples)],
                'birth_date': pd.to_datetime('2000-01-01') - pd.to_timedelta(np.random.randint(20*365, 60*365, n_samples), unit='D'),
                'hire_date': pd.to_datetime('2023-01-01') - pd.to_timedelta(np.random.randint(1*365, 10*365, n_samples), unit='D')
            })
            df['birth_date'] = df['birth_date'].dt.strftime('%Y-%m-%d')
            df['hire_date'] = df['hire_date'].dt.strftime('%Y-%m-%d')

        elif dataset_name == "medical_records":
            n_samples = 1200 * size_multiplier
            # ... (rest of medical_records generation code)
            conditions = ['Diabetes', 'Hypertension', 'Asthma', 'Heart Disease', 'None']
            df = pd.DataFrame({
                'patient_id': [f"P{str(i).zfill(8)}" for i in range(1, n_samples + 1)],
                'patient_name': [f"Patient {i}" for i in range(n_samples)],
                'age': np.random.gamma(2, 30, n_samples).clip(1, 100).astype(int),
                'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples, p=[0.48, 0.50, 0.02]),
                'diagnosis': np.random.choice(conditions, n_samples, p=[0.15, 0.20, 0.15, 0.10, 0.40]),
                'treatment_cost': np.random.exponential(5000, n_samples).round(2),
                'insurance_id': [f"INS{np.random.randint(100000, 999999)}" for _ in range(n_samples)],
                'doctor_name': np.random.choice(['Dr. Smith', 'Dr. Johnson', 'Dr. Williams', 'Dr. Brown'], n_samples),
                'visit_date': pd.to_datetime('2023-01-01') - pd.to_timedelta(np.random.randint(1, 3*365, n_samples), unit='D'),
                'blood_pressure': [f"{np.random.randint(90, 180)}/{np.random.randint(60, 120)}" for _ in range(n_samples)]
            })
            df['visit_date'] = df['visit_date'].dt.strftime('%Y-%m-%d')
            
        elif dataset_name == "financial_transactions":
            n_samples = 2000 * size_multiplier
            # ... (rest of financial_transactions generation code)
            transaction_types = ['Purchase', 'Transfer', 'Deposit', 'Withdrawal', 'Payment']
            df = pd.DataFrame({
                'transaction_id': [f"TXN{str(i).zfill(10)}" for i in range(1, n_samples + 1)],
                'account_number': [f"{np.random.randint(1000000000, 9999999999)}" for _ in range(n_samples)],
                'customer_name': [f"Customer {i}" for i in range(n_samples)],
                'transaction_type': np.random.choice(transaction_types, n_samples),
                'amount': np.random.exponential(500, n_samples).round(2),
                'balance_after': np.random.exponential(10000, n_samples).round(2),
                'merchant': np.random.choice(['Amazon', 'Walmart', 'Target', 'Starbucks', 'Other'], n_samples),
                'location_zip_prefix': np.random.randint(100, 999, n_samples).astype(str),
                'timestamp': pd.to_datetime('2023-01-01 00:00:00') + pd.to_timedelta(np.random.randint(0, 365*24*60*60, n_samples), unit='s'),
                'card_last_four': [f"{np.random.randint(1000, 9999)}" for _ in range(n_samples)]
            })
            df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')

        elif dataset_name == "survey_responses":
            n_samples = 1500 * size_multiplier
            # ... (rest of survey_responses generation code)
            df = pd.DataFrame({
                'response_id': range(1, n_samples + 1),
                'respondent_email_hash': [hash(f"resp{i}@survey.com") % (10**8) for i in range(n_samples)],
                'age_group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '56-65', '65+'], n_samples),
                'income_bracket': np.random.choice(['<30K', '30-50K', '50-75K', '75-100K', '>150K'], n_samples),
                'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),
                'satisfaction_score': np.random.choice([1, 2, 3, 4, 5], n_samples),
                'feedback_keywords': [",".join(np.random.choice(['good', 'bad', 'neutral', 'improve', 'great', 'poor'], np.random.randint(1,4))) for _ in range(n_samples)],
                'submission_timestamp': pd.to_datetime('2023-06-01 00:00:00') + pd.to_timedelta(np.random.randint(0, 180*24*60*60, n_samples), unit='s'),
                'ip_octet_1_2': [f"{np.random.randint(1, 255)}.{np.random.randint(1, 255)}" for _ in range(n_samples)]
            })
            df['submission_timestamp'] = df['submission_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
            
        elif dataset_name == "clinical_trials": # Existing
            n_samples = 950 * size_multiplier
            df = pd.DataFrame({
                'trial_id': [f"CT{str(i).zfill(7)}" for i in range(1, n_samples + 1)],
                'participant_ssn_last4': [f"{np.random.randint(1000, 9999)}" for _ in range(n_samples)],
                'participant_initials': [f"{chr(65+np.random.randint(0,25))}{chr(65+np.random.randint(0,25))}" for _ in range(n_samples)],
                'birth_year': np.random.randint(1950, 1996, n_samples),
                'trial_drug_group': np.random.choice(['DrugA', 'DrugB', 'Placebo'], n_samples, p=[0.4, 0.4, 0.2]),
                'dosage_mg': np.random.choice([10, 25, 50, 100], n_samples),
                'blood_type_group': np.random.choice(['A', 'B', 'AB', 'O'], n_samples), # Simplified
                'has_preexisting_condition': np.random.choice([True, False], n_samples, p=[0.3, 0.7]),
                'emergency_contact_relation': np.random.choice(['Spouse', 'Parent', 'Sibling', 'Friend'], n_samples),
                'physician_id': [f"DOC{np.random.randint(100,199)}" for _ in range(n_samples)]
            })
        else:
            st.error(f"Unknown dataset: {dataset_name}")
            return None
            
        return df
        
    except Exception as e:
        st.error(f"Error loading dataset {dataset_name}: {e}\n{traceback.format_exc()}")
        return None

def get_dataset_info(dataset_name, dataset_size="sample"):
    """Get information about the dataset with size-specific details"""
    
    base_info = {
        # Scikit-learn Classic
        "iris": {"description": "Iris flower species classification (3 classes). Famous ML dataset.", "base_samples": 150, "features": 4, "target_cols": ["target"], "sensitive_columns": [], "recommended_techniques": ["Classification-aware methods"]},
        "wine": {"description": "Wine recognition dataset; chemical analysis of wines (3 classes).", "base_samples": 178, "features": 13, "target_cols": ["target"], "sensitive_columns": [], "recommended_techniques": ["Generalization"]},
        "breast_cancer": {"description": "Breast cancer diagnosis (binary classification). Features are computed from digitized image of a fine needle aspirate (FNA) of a breast mass.", "base_samples": 569, "features": 30, "target_cols": ["target"], "sensitive_columns": ["target"], "recommended_techniques": ["Suppression", "Randomization"]},
        "diabetes": {"description": "Diabetes progression one year after baseline. Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements.", "base_samples": 442, "features": 10, "target_cols": ["target"], "sensitive_columns": ["age", "sex"], "recommended_techniques": ["Numeric Perturbation", "Aggregation"]},
        "digits": {"description": "Handwritten digits recognition (10 classes). Each feature is an 8x8 image of a digit.", "base_samples": 1797, "features": 64, "target_cols": ["target"], "sensitive_columns": [], "recommended_techniques": ["Dimensionality Reduction based Anonymization"]},
        "california_housing": {"description": "California housing prices. Based on the 1990 California census data.", "base_samples": 20640, "features": 8, "target_cols": ["MedHouseVal"], "sensitive_columns": ["MedInc", "HouseAge", "AveRooms"], "recommended_techniques": ["Microaggregation", "Top/Bottom Coding"]},

        # Synthetic ML Archetypes
        "boston_synthetic": {"description": "Synthetic regression dataset inspired by Boston Housing.", "base_samples": 506, "features": 13 + 2, "target_cols": ["target_price"], "sensitive_columns": ["owner_name_initials", "street_number_hash"], "recommended_techniques": ["Redaction", "Hashing"]},
        "credit_approval_synthetic": {"description": "Synthetic binary classification for credit approval.", "base_samples": 690, "features": 15 + 2, "target_cols": ["approved"], "sensitive_columns": ["applicant_id", "zip_code_prefix", "attr_1", "attr_2"], "recommended_techniques": ["Pseudonymization", "Suppression"]},
        "marketing_campaign_synthetic": {"description": "Synthetic binary classification for marketing campaign subscription.", "base_samples": 2240, "features": 10 + 2, "target_cols": ["subscribed"], "sensitive_columns": ["customer_email_domain", "last_contact_days_ago"], "recommended_techniques": ["Generalization", "Redaction"]},
        "flower_species_synthetic": {"description": "Synthetic multi-class classification for flower species.", "base_samples": 300, "features": 4 + 1, "target_cols": ["species_type"], "sensitive_columns": ["observation_id"], "recommended_techniques": ["k-Anonymity"]},
        "music_genre_synthetic": {"description": "Synthetic multi-class classification for music genres.", "base_samples": 1000, "features": 20 + 1, "target_cols": ["genre"], "sensitive_columns": ["track_id_suffix", "audio_feature_1"], "recommended_techniques": ["l-Diversity"]},
        "stock_prices_synthetic": {"description": "Synthetic regression for stock price changes.", "base_samples": 500, "features": 5 + 2, "target_cols": ["price_change"], "sensitive_columns": ["company_ticker_generic", "trade_date_offset"], "recommended_techniques": ["Noise Addition", "Aggregation"]},

        # OpenML Datasets
        "titanic": {"description": "Titanic passenger survival data (binary classification).", "base_samples": 1309, "features": 13, "target_cols": ["survived"], "sensitive_columns": ["name", "ticket", "cabin", "age"], "recommended_techniques": ["Redaction", "Categorization"]},
        "heart_disease_cleveland": {"description": "Cleveland Heart Disease dataset for predicting presence of heart disease.", "base_samples": 303, "features": 13, "target_cols": ["class"], "sensitive_columns": ["age", "sex"], "recommended_techniques": ["Suppression", "Generalization"]},
        "auto_mpg": {"description": "Predicting city-cycle fuel consumption in miles per gallon.", "base_samples": 398, "features": 8, "target_cols": ["mpg"], "sensitive_columns": ["name", "origin"], "recommended_techniques": ["Rounding", "Top/Bottom Coding"]},
        "adult_income": {"description": "Predict whether income exceeds $50K/yr based on census data (Adult dataset).", "base_samples": 48842, "features": 14, "target_cols": ["class"], "sensitive_columns": ["age", "education", "marital-status", "occupation", "race", "sex", "native-country"], "recommended_techniques": ["Generalization", "Suppression", "k-Anonymity"]},
        
        # Existing Synthetic Datasets
        "customer_data": {
            "description": "Customer database with PII and financial information.",
            "base_samples": 1000, "features": 10, 
            "sensitive_columns": ["first_name", "last_name", "phone_number", "email", "zip_code"],
            "recommended_techniques": ["Basic Redaction", "Numeric Scaler", "Pseudonymization"]
        },
        "employee_records": {
            "description": "Employee HR records with salaries and personal data.",
            "base_samples": 800, "features": 10, 
            "sensitive_columns": ["full_name", "ssn", "birth_date", "salary"],
            "recommended_techniques": ["Basic Redaction", "Numeric Perturbation", "Hashing"]
        },
        "medical_records": {
            "description": "Patient medical records with health information.",
            "base_samples": 1200, "features": 10, 
            "sensitive_columns": ["patient_name", "insurance_id", "doctor_name", "blood_pressure", "diagnosis"],
            "recommended_techniques": ["Basic Redaction", "Generalization", "Suppression"]
        },
        "financial_transactions": {
            "description": "Bank transaction records with account details.",
            "base_samples": 2000, "features": 10, 
            "sensitive_columns": ["account_number", "customer_name", "card_last_four", "balance_after"],
            "recommended_techniques": ["Basic Redaction", "Masking", "Noise Addition"]
        },
        "survey_responses": {
            "description": "Survey data with demographic and contact information.",
            "base_samples": 1500, "features": 9,
            "sensitive_columns": ["respondent_email_hash", "ip_octet_1_2", "feedback_keywords"],
            "recommended_techniques": ["Basic Redaction", "Aggregation", "Top/Bottom Coding"]
        },
        "clinical_trials": {
            "description": "Clinical trial participant data with medical history.",
            "base_samples": 950, "features": 10, 
            "sensitive_columns": ["participant_ssn_last4", "participant_initials", "birth_year", "emergency_contact_relation"],
            "recommended_techniques": ["Basic Redaction", "Pseudonymization", "Generalization"]
        }
    }
    
    info = base_info.get(dataset_name, {})
    if not info:
        return {}
    
    # Calculate actual samples based on size multiplier
    size_multipliers = {"sample": 1, "medium": 3, "full": 10}
    multiplier = size_multipliers.get(dataset_size, 1)
    
    info = info.copy()

    # For large datasets, don't multiply base_samples
    if dataset_name not in ["california_housing", "adult_income"]:
        info["samples"] = info["base_samples"] * multiplier
    else:
        info["samples"] = info["base_samples"]

    info["dataset_size"] = dataset_size.title()
    
    return info

# Update the main area display when sample dataset is loaded:

# Add sample dataset info display if loaded
if (hasattr(st.session_state, 'sample_dataset_info') and 
    hasattr(st.session_state, 'loaded_sample_dataset_name') and
    st.session_state.sample_dataset_info and
    st.session_state.persisted_uploaded_file_bytes is not None):
    
    st.info(f"ğŸ“Š **Sample Dataset Loaded:** {st.session_state.loaded_sample_dataset_name}")
    
    # Show dataset overview WITH SIZE INFO
    info = st.session_state.sample_dataset_info
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("ğŸ“ Samples", f"{info.get('samples', 0):,}")
    with col2:
        st.metric("ğŸ“Š Features", info.get('features', 0))
    with col3:
        st.metric("ğŸ”’ Sensitive Columns", len(info.get('sensitive_columns', [])))
    with col4:
        st.metric("ğŸ“¦ Dataset Size", info.get('dataset_size', 'sample').title())
    with col5:
        if st.button("ğŸ—‘ï¸ Clear", help="Clear sample dataset"):
            # Clear logic here...
            st.rerun()
    
    # Show performance expectations
    size_key = info.get('dataset_size', 'sample')
    if size_key == "sample":
        st.success("âš¡ **Fast Processing:** This sample size is optimized for quick testing and learning.")
    elif size_key == "medium":
        st.warning("â±ï¸ **Moderate Processing:** This medium size tests performance with realistic data volumes.")
    else:
        st.error("ğŸŒ **Slower Processing:** This full size simulates real-world data volumes. Processing may take longer.")
    
    # Show sensitive columns info with size context
    if info.get('sensitive_columns'):
        with st.expander("ğŸ”’ Sensitive Columns Details", expanded=False):
            st.write("**Columns containing sensitive information:**")
            for col in info['sensitive_columns']:
                st.write(f"â€¢ **{col}**")
            st.write(f"**ğŸ’¡ Tip:** Consider applying anonymization to these columns using techniques like {', '.join(info.get('recommended_techniques', ['Basic Redaction']))}.")
            st.write(f"**ğŸ“Š Scale:** With {info.get('samples', 0):,} rows, this {size_key} dataset will help you understand anonymization performance at different scales.")
            
# Add this code after the line that shows the dataset count
# (after: st.sidebar.caption(f"ğŸ“‹ Showing {total_datasets} datasets in {selected_category}"))

# Dataset selection dropdown - THIS IS MISSING!
selected_dataset = st.sidebar.selectbox(
    "Choose a sample dataset:",
    options=list(sample_dataset_options.keys()),
    key="sample_dataset_select",
    help="Select a built-in dataset to test anonymization techniques"
)

if selected_dataset != "None":
    dataset_key = sample_dataset_options[selected_dataset]
    
    # Show quick info about the dataset WITH SIZE INFO
    info = get_dataset_info(dataset_key, dataset_size_key)
    if info:
        with st.sidebar.expander(f"ğŸ“‹ About {selected_dataset} ({selected_size})", expanded=False):
            col1, col2 = st.sidebar.columns(2)
            with col1:
                st.write(f"**ğŸ“ Samples:** {info.get('samples', 'N/A'):,}")
                st.write(f"**ğŸ“Š Features:** {info.get('features', 'N/A')}")
            with col2:
                st.write(f"**ğŸ”’ Sensitive Cols:** {len(info.get('sensitive_columns', []))}")
                st.write(f"**ğŸ“¦ Size:** {info.get('dataset_size', 'sample').title()}")
            
            st.write(f"**ğŸ“ Description:** {info.get('description', 'No description available')}")
            if info.get('sensitive_columns'):
                st.write(f"**ğŸ”’ Sensitive Fields:** {', '.join(info['sensitive_columns'][:3])}{'...' if len(info['sensitive_columns']) > 3 else ''}")
            if info.get('recommended_techniques'):
                st.write(f"**ğŸ’¡ Recommended:** {', '.join(info['recommended_techniques'])}")
    
    # Load button WITH SIZE INDICATOR
    col1, col2 = st.sidebar.columns([2, 1])
    with col1:
        load_button_text = f"ğŸ“¥ Load {selected_size.split('(')[0].strip()}"
        if st.sidebar.button(load_button_text, type="secondary", key="load_sample_btn"):
            with st.spinner(f"Loading {selected_dataset} ({dataset_size_key} size)..."):
                # Load the dataset WITH SIZE PARAMETER
                sample_df = load_sample_dataset(dataset_key, dataset_size_key)
                
                if sample_df is not None:
                    # Clear any existing data
                    st.session_state.persisted_uploaded_file_bytes = None
                    st.session_state.persisted_uploaded_file_name = None
                    st.session_state.last_uploader_key_persisted = None
                    st.session_state.df_anonymized_data = None
                    st.session_state.df_anonymized_source_technique = None
                    
                    # Store sample dataset as if it were uploaded
                    csv_bytes = sample_df.to_csv(index=False).encode('utf-8')
                    st.session_state.persisted_uploaded_file_bytes = csv_bytes
                    st.session_state.persisted_uploaded_file_name = f"{dataset_key}_{dataset_size_key}.csv"
                    st.session_state.last_uploader_key_persisted = "file_uploader_main_data"
                    
                    # Store dataset info for display (with size info)
                    st.session_state.sample_dataset_info = info
                    st.session_state.loaded_sample_dataset_name = f"{selected_dataset} ({dataset_size_key.title()})"
                    
                    st.sidebar.success(f"âœ… {selected_dataset} ({dataset_size_key}) loaded successfully!")
                    st.sidebar.info(f"ğŸ“Š Loaded {len(sample_df):,} rows Ã— {len(sample_df.columns)} columns")
                    st.rerun()
                else:
                    st.sidebar.error(f"âŒ Failed to load {selected_dataset}")
    
    with col2:
        if st.sidebar.button("ğŸ”", key="preview_sample_btn", help="Quick preview"):
            with st.spinner("Loading preview..."):
                preview_df = load_sample_dataset(dataset_key, dataset_size_key)
                if preview_df is not None:
                    st.sidebar.write(f"**ğŸ‘€ Preview: {selected_dataset} ({dataset_size_key})**")
                    st.sidebar.dataframe(preview_df.head(5), use_container_width=True)
                    st.sidebar.caption(f"Showing first 5 rows of {len(preview_df):,} total rows")

# Show loaded sample dataset info WITH SIZE
if (hasattr(st.session_state, 'sample_dataset_info') and 
    hasattr(st.session_state, 'loaded_sample_dataset_name') and
    st.session_state.sample_dataset_info):
    
    info = st.session_state.sample_dataset_info
    st.sidebar.success(f"ğŸ“Š Dataset loaded: **{st.session_state.loaded_sample_dataset_name}**")
    
    # Quick stats with size info
    col1, col2 = st.sidebar.columns(2)
    with col1:
        st.metric("ğŸ“ Samples", f"{info.get('samples', 0):,}")
        st.metric("ğŸ”’ Sensitive", len(info.get('sensitive_columns', [])))
    with col2:
        st.metric("ğŸ“¦ Size", info.get('dataset_size', 'sample').title())
        
        # Performance indicator
        size_key = info.get('dataset_size', 'sample')
        if size_key == "sample":
            st.caption("âš¡ Fast processing")
        elif size_key == "medium":  
            st.caption("â±ï¸ Moderate processing")
        else:
            st.caption("ğŸŒ Slower processing")
    
    # Download button for original dataset
    if (st.session_state.persisted_uploaded_file_bytes and 
        st.session_state.persisted_uploaded_file_name):
        st.sidebar.download_button(
            label="ğŸ“¥ Download Original Dataset",
            data=st.session_state.persisted_uploaded_file_bytes,
            file_name=st.session_state.persisted_uploaded_file_name,
            mime="text/csv",
            key="download_original_sample_dataset",
            help="Download the loaded original dataset for future use",
            use_container_width=True
        )
    
    # Recommended techniques
    if info.get('recommended_techniques'):
        st.sidebar.info(f"ğŸ’¡ **Recommended techniques:** {', '.join(info['recommended_techniques'])}")

# === ML APP SESSION STATE BRIDGE STATUS ===
st.sidebar.markdown("---")
st.sidebar.header("ğŸ”— ML App Integration")

# Show current ML app dataset status
if st.session_state.datasets_collection.get('original') is not None:
    st.sidebar.success("âœ… **ML App Ready**")
    
    # Original dataset info
    original_df = st.session_state.datasets_collection['original']
    st.sidebar.write(f"**ğŸ“Š Original Dataset:** {original_df.shape[0]} rows, {original_df.shape[1]} columns")
    
    # Anonymized datasets info
    total_anonymized = sum(len(datasets) for datasets in st.session_state.datasets_collection['anonymized_datasets'].values())
    st.sidebar.write(f"**ğŸ”’ Anonymized Datasets:** {total_anonymized} loaded")
    
    if st.session_state.datasets_collection['anonymized_datasets']:
        with st.sidebar.expander("View Anonymized Datasets", expanded=False):
            for technique, datasets in st.session_state.datasets_collection['anonymized_datasets'].items():
                st.write(f"**{technique}:** {len(datasets)} dataset(s)")
                for i, dataset in enumerate(datasets, 1):
                    st.caption(f"  {i}. {dataset['dataset_id']} ({dataset['timestamp']})")
    
    # Quick actions
    col1, col2 = st.sidebar.columns([1, 1])
    with col1:
        if st.button("ğŸš€ Open ML App", key="open_ml_app", help="Reminder to run ML app"):
            st.info("ğŸ’¡ Run in terminal: `streamlit run ml_app.py`")
    
    with col2:
        if st.button("ğŸ—‘ï¸ Clear ML Data", key="clear_ml_data", help="Clear all ML app data"):
            st.session_state.datasets_collection = {'original': None, 'anonymized_datasets': {}}
            st.session_state.dataset_counters = {}
            st.session_state.dataset_metadata = {}
            st.session_state.ml_transfer_status = None
            st.success("ML app data cleared")
            st.rerun()

else:
    st.sidebar.info("ğŸ”„ **ML App Status:** No datasets transferred yet")
    st.sidebar.caption("Use the 'Transfer to ML App' button after anonymization to send datasets to the ML experimentation platform")

# Show last transfer status
if st.session_state.ml_transfer_status:
    status = st.session_state.ml_transfer_status
    if status['success']:
        st.sidebar.success(f"âœ… Last transfer: {status['technique']} ({status['timestamp']})")
    else:
        st.sidebar.error(f"âŒ Last transfer failed ({status['timestamp']})")
